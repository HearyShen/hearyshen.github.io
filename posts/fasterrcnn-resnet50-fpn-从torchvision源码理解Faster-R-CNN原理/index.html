<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="baidu-site-verification" content="WmzZoAIhtw8L5PpX">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"heary.cn","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"post_block":"zoomIn","post_header":"zoomIn","post_body":"fadeInUp","coll_header":"fadeInLeft","sidebar":"zoomIn"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="PyTorch的torchvision包中实现了Faster R-CNN。本文结合对torchvision源码的阅读，深入理解Faster R-CNN的内部原理，以便进行开发利用。">
<meta property="og:type" content="article">
<meta property="og:title" content="fasterrcnn_resnet50_fpn - 从torchvision源码理解Faster R-CNN原理">
<meta property="og:url" content="https://heary.cn/posts/fasterrcnn-resnet50-fpn-%E4%BB%8Etorchvision%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3Faster-R-CNN%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="Heary&#39;s Blog">
<meta property="og:description" content="PyTorch的torchvision包中实现了Faster R-CNN。本文结合对torchvision源码的阅读，深入理解Faster R-CNN的内部原理，以便进行开发利用。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-11-25T11:41:12.000Z">
<meta property="article:modified_time" content="2022-08-07T04:02:09.859Z">
<meta property="article:author" content="Heary">
<meta property="article:tag" content="R-CNN">
<meta property="article:tag" content="Object Detection">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Faster R-CNN">
<meta property="article:tag" content="torchvision">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://heary.cn/posts/fasterrcnn-resnet50-fpn-%E4%BB%8Etorchvision%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3Faster-R-CNN%E5%8E%9F%E7%90%86/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://heary.cn/posts/fasterrcnn-resnet50-fpn-%E4%BB%8Etorchvision%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3Faster-R-CNN%E5%8E%9F%E7%90%86/","path":"posts/fasterrcnn-resnet50-fpn-从torchvision源码理解Faster-R-CNN原理/","title":"fasterrcnn_resnet50_fpn - 从torchvision源码理解Faster R-CNN原理"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>fasterrcnn_resnet50_fpn - 从torchvision源码理解Faster R-CNN原理 | Heary's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-135282529-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-135282529-1","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?93bca42dda78417b488ac006a6ac5444"></script>


  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;e56a1ace8bf142f3b73bedc96953a959&quot;}'></script>


  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Heary's Blog" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="Heary's Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Heary's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">渡口缀满灯花</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">186</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">146</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#fasterrcnn_resnet50_fpn---%E4%BB%8Etorchvision%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3faster-r-cnn%E5%8E%9F%E7%90%86"><span class="nav-text">fasterrcnn_resnet50_fpn
- 从torchvision源码理解Faster R-CNN原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A5%E5%8F%A3%E5%B1%82"><span class="nav-text">1 接口层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%96%E9%83%A8%E8%B0%83%E7%94%A8"><span class="nav-text">外部调用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fasterrcnn_resnet50_fpn"><span class="nav-text">fasterrcnn_resnet50_fpn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#resnet_fpn_backbone"><span class="nav-text">resnet_fpn_backbone</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%B1%82"><span class="nav-text">2 实现层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#fasterrcnn"><span class="nav-text">FasterRCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#generalizedrcnn"><span class="nav-text">GeneralizedRCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#generalizedrcnntransform"><span class="nav-text">GeneralizedRCNNTransform</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#backbonewithfpn"><span class="nav-text">BackboneWithFPN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#resnet"><span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fpnfeaturepyramidnetwork"><span class="nav-text">FPN(FeaturePyramidNetwork)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#regionproposalnetwork"><span class="nav-text">RegionProposalNetwork</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#anchorgenerator"><span class="nav-text">AnchorGenerator</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rpnhead"><span class="nav-text">RPNHead</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#boxcoder"><span class="nav-text">BoxCoder</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#filter_proposals"><span class="nav-text">filter_proposals</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roiheads"><span class="nav-text">RoIHeads</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#multiscaleroialign"><span class="nav-text">MultiScaleRoIAlign</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#twomlphead"><span class="nav-text">TwoMLPHead</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fastrcnnpredictor"><span class="nav-text">FastRCNNPredictor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#postprocess_detections"><span class="nav-text">postprocess_detections</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#generalizedrcnntransform.postprocess"><span class="nav-text">GeneralizedRCNNTransform.postprocess</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">3 总结</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Heary"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Heary</p>
  <div class="site-description" itemprop="description">养天地正气 法古今完人</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">146</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">186</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/HearyShen" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HearyShen" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/jiayun-shen/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;jiayun-shen&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jiayun.shen@foxmail.com" title="E-Mail → mailto:jiayun.shen@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://heary.cn/" title="https:&#x2F;&#x2F;heary.cn">Heary's Blog(Main)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hearyshen.github.io/" title="https:&#x2F;&#x2F;hearyshen.github.io" rel="noopener" target="_blank">Heary's Blog(Mirror)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://lonelyone.cn/" title="http:&#x2F;&#x2F;lonelyone.cn" rel="noopener" target="_blank">Lonelyone.cn</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://piaoyu.org/" title="http:&#x2F;&#x2F;piaoyu.org" rel="noopener" target="_blank">piaoyu.org</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://transformerswsz.github.io/" title="https:&#x2F;&#x2F;transformerswsz.github.io" rel="noopener" target="_blank">Swift的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://log4me.github.io/" title="https:&#x2F;&#x2F;log4me.github.io&#x2F;" rel="noopener" target="_blank">logme's blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://lossherl.github.io/" title="https:&#x2F;&#x2F;lossherl.github.io&#x2F;" rel="noopener" target="_blank">Sherl's</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

  <a href="https://github.com/HearyShen" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://heary.cn/posts/fasterrcnn-resnet50-fpn-%E4%BB%8Etorchvision%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3Faster-R-CNN%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Heary">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heary's Blog">
      <meta itemprop="description" content="养天地正气 法古今完人">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="fasterrcnn_resnet50_fpn - 从torchvision源码理解Faster R-CNN原理 | Heary's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          fasterrcnn_resnet50_fpn - 从torchvision源码理解Faster R-CNN原理
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-25 19:41:12" itemprop="dateCreated datePublished" datetime="2020-11-25T19:41:12+08:00">2020-11-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-08-07 12:02:09" itemprop="dateModified" datetime="2022-08-07T12:02:09+08:00">2022-08-07</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>96k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:27</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>PyTorch的torchvision包中实现了Faster
R-CNN。本文结合对torchvision源码的阅读，深入理解Faster
R-CNN的内部原理，以便进行开发利用。</p>
<span id="more"></span>
<h1
id="fasterrcnn_resnet50_fpn---从torchvision源码理解faster-r-cnn原理">fasterrcnn_resnet50_fpn
- 从torchvision源码理解Faster R-CNN原理</h1>
<h2 id="接口层">1 接口层</h2>
<h3 id="外部调用">外部调用</h3>
<p>根据PyTorch的torchvision库的文档，Faster
R-CNN模型对象可以直接通过<code>fasterrcnn_resnet50_fpn</code>函数来构造。</p>
<p>具体地，官方文档给出了训练时和预测时的调用样例：</p>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torchvision/models.html?highlight=faster%20rcnn#torchvision.models.detection.fasterrcnn_resnet50_fpn">torchvision.models.detection.fasterrcnn_resnet50_fpn</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># For training</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>images, boxes = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">600</span>, <span class="number">1200</span>), torch.rand(<span class="number">4</span>, <span class="number">11</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>labels = torch.randint(<span class="number">1</span>, <span class="number">91</span>, (<span class="number">4</span>, <span class="number">11</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>images = <span class="built_in">list</span>(image <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>targets = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(images)):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    d = &#123;&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    d[<span class="string">&#x27;boxes&#x27;</span>] = boxes[i]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    d[<span class="string">&#x27;labels&#x27;</span>] = labels[i]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    targets.append(d)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = model(images, targets)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># For inference</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = [torch.rand(<span class="number">3</span>, <span class="number">300</span>, <span class="number">400</span>), torch.rand(<span class="number">3</span>, <span class="number">500</span>, <span class="number">400</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>predictions = model(x)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># optionally, if you want to export the model to ONNX:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.onnx.export(model, x, <span class="string">&quot;faster_rcnn.onnx&quot;</span>, opset_version = <span class="number">11</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>其中，不论是训练，还是预测，模型的输入都是list容器，表示的是若干个图片（与目标框和类别）。</li>
</ul>
<h3 id="fasterrcnn_resnet50_fpn">fasterrcnn_resnet50_fpn</h3>
<p><code>fasterrcnn_resnet50_fpn</code>函数在<code>torchvision.models.detection.faster_rcnn</code>包中实现，文档见<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn">torchvision.models.detection.fasterrcnn_resnet50_fpn</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fasterrcnn_resnet50_fpn</span>(<span class="params">pretrained=<span class="literal">False</span>, progress=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                            num_classes=<span class="number">91</span>, pretrained_backbone=<span class="literal">True</span>, trainable_backbone_layers=<span class="number">3</span>, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each</span></span><br><span class="line"><span class="string">    image, and should be in ``0-1`` range. Different images can have different sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The behavior of the model changes depending if it is in training or evaluation mode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training, the model expects both the input tensors, as well as a targets (list of dictionary),</span></span><br><span class="line"><span class="string">    containing:</span></span><br><span class="line"><span class="string">        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with values of ``x``</span></span><br><span class="line"><span class="string">          between ``0`` and ``W`` and values of ``y`` between ``0`` and ``H``</span></span><br><span class="line"><span class="string">        - labels (``Int64Tensor[N]``): the class label for each ground-truth box</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression</span></span><br><span class="line"><span class="string">    losses for both the RPN and the R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During inference, the model requires only the input tensors, and returns the post-processed</span></span><br><span class="line"><span class="string">    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as</span></span><br><span class="line"><span class="string">    follows:</span></span><br><span class="line"><span class="string">        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with values of ``x``</span></span><br><span class="line"><span class="string">          between ``0`` and ``W`` and values of ``y`` between ``0`` and ``H``</span></span><br><span class="line"><span class="string">        - labels (``Int64Tensor[N]``): the predicted labels for each image</span></span><br><span class="line"><span class="string">        - scores (``Tensor[N]``): the scores or each prediction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # For training</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; labels = torch.randint(1, 91, (4, 11))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; images = list(image for image in images)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; targets = []</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; for i in range(len(images)):</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;     d = &#123;&#125;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;     d[&#x27;boxes&#x27;] = boxes[i]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;     d[&#x27;labels&#x27;] = labels[i]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;     targets.append(d)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = model(images, targets)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # For inference</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model.eval()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; predictions = model(x)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # optionally, if you want to export the model to ONNX:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; torch.onnx.export(model, x, &quot;faster_rcnn.onnx&quot;, opset_version = 11)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on COCO train2017</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet</span></span><br><span class="line"><span class="string">        num_classes (int): number of output classes of the model (including the background)</span></span><br><span class="line"><span class="string">        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.</span></span><br><span class="line"><span class="string">            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> trainable_backbone_layers &lt;= <span class="number">5</span> <span class="keyword">and</span> trainable_backbone_layers &gt;= <span class="number">0</span></span><br><span class="line">    <span class="comment"># dont freeze any layers if pretrained model or backbone is not used</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (pretrained <span class="keyword">or</span> pretrained_backbone):</span><br><span class="line">        trainable_backbone_layers = <span class="number">5</span></span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        <span class="comment"># no need to download the backbone if pretrained is set</span></span><br><span class="line">        pretrained_backbone = <span class="literal">False</span></span><br><span class="line">    backbone = resnet_fpn_backbone(<span class="string">&#x27;resnet50&#x27;</span>, pretrained_backbone, trainable_layers=trainable_backbone_layers)</span><br><span class="line">    model = FasterRCNN(backbone, num_classes, **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(model_urls[<span class="string">&#x27;fasterrcnn_resnet50_fpn_coco&#x27;</span>],</span><br><span class="line">                                              progress=progress)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>该函数的实现中，首先进行参数检查：</p>
<ol type="1">
<li>检查<code>trainable_backbone_layers</code>参数，必须在0~5之间，表示从最后一层开始计数，有几层在训练中是可优化的；</li>
<li>检查<code>pretrained</code>和<code>pretrained_backbone</code>参数，如果整个模型都设为预训练的，那就当然没必要再单独下载预训练的<code>backbone</code>了，把整个Faster
R-CNN模型都载入预训练参数即可。</li>
</ol>
<p>Faster
R-CNN模型是<code>FasterRCNN</code>类的实例。实例化时，传入指定的<code>backbone</code>作为<code>FasterRCNN</code>的backbone。</p>
<h3 id="resnet_fpn_backbone">resnet_fpn_backbone</h3>
<p>backbone通过对外开放的<code>resnet_fpn_backbone</code>函数来构造。</p>
<p><code>resnet_fpn_backbone</code>函数在<code>torchvision.models.detection.backbone_utils</code>包中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_fpn_backbone</span>(<span class="params"></span></span><br><span class="line"><span class="params">    backbone_name,</span></span><br><span class="line"><span class="params">    pretrained,</span></span><br><span class="line"><span class="params">    norm_layer=misc_nn_ops.FrozenBatchNorm2d,</span></span><br><span class="line"><span class="params">    trainable_layers=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">    returned_layers=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    extra_blocks=<span class="literal">None</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Constructs a specified ResNet backbone with FPN on top. Freezes the specified number of layers in the backbone.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; from torchvision.models.detection.backbone_utils import resnet_fpn_backbone</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; backbone = resnet_fpn_backbone(&#x27;resnet50&#x27;, pretrained=True, trainable_layers=3)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # get some dummy image</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x = torch.rand(1,3,64,64)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # compute the output</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = backbone(x)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print([(k, v.shape) for k, v in output.items()])</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # returns</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;   [(&#x27;0&#x27;, torch.Size([1, 256, 16, 16])),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;    (&#x27;1&#x27;, torch.Size([1, 256, 8, 8])),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;    (&#x27;2&#x27;, torch.Size([1, 256, 4, 4])),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;    (&#x27;3&#x27;, torch.Size([1, 256, 2, 2])),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;    (&#x27;pool&#x27;, torch.Size([1, 256, 1, 1]))]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        backbone_name (string): resnet architecture. Possible values are &#x27;ResNet&#x27;, &#x27;resnet18&#x27;, &#x27;resnet34&#x27;, &#x27;resnet50&#x27;,</span></span><br><span class="line"><span class="string">             &#x27;resnet101&#x27;, &#x27;resnet152&#x27;, &#x27;resnext50_32x4d&#x27;, &#x27;resnext101_32x8d&#x27;, &#x27;wide_resnet50_2&#x27;, &#x27;wide_resnet101_2&#x27;</span></span><br><span class="line"><span class="string">        norm_layer (torchvision.ops): it is recommended to use the default value. For details visit:</span></span><br><span class="line"><span class="string">            (https://github.com/facebookresearch/maskrcnn-benchmark/issues/267)</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model with backbone pre-trained on Imagenet</span></span><br><span class="line"><span class="string">        trainable_layers (int): number of trainable (not frozen) resnet layers starting from final block.</span></span><br><span class="line"><span class="string">            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    backbone = resnet.__dict__[backbone_name](</span><br><span class="line">        pretrained=pretrained,</span><br><span class="line">        norm_layer=norm_layer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># select layers that wont be frozen</span></span><br><span class="line">    <span class="keyword">assert</span> trainable_layers &lt;= <span class="number">5</span> <span class="keyword">and</span> trainable_layers &gt;= <span class="number">0</span></span><br><span class="line">    layers_to_train = [<span class="string">&#x27;layer4&#x27;</span>, <span class="string">&#x27;layer3&#x27;</span>, <span class="string">&#x27;layer2&#x27;</span>, <span class="string">&#x27;layer1&#x27;</span>, <span class="string">&#x27;conv1&#x27;</span>][:trainable_layers]</span><br><span class="line">    <span class="comment"># freeze layers only if pretrained backbone is used</span></span><br><span class="line">    <span class="keyword">for</span> name, parameter <span class="keyword">in</span> backbone.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>([<span class="keyword">not</span> name.startswith(layer) <span class="keyword">for</span> layer <span class="keyword">in</span> layers_to_train]):</span><br><span class="line">            parameter.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> extra_blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        extra_blocks = LastLevelMaxPool()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> returned_layers <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        returned_layers = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">min</span>(returned_layers) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">max</span>(returned_layers) &lt; <span class="number">5</span></span><br><span class="line">    return_layers = &#123;<span class="string">f&#x27;layer<span class="subst">&#123;k&#125;</span>&#x27;</span>: <span class="built_in">str</span>(v) <span class="keyword">for</span> v, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(returned_layers)&#125;</span><br><span class="line"></span><br><span class="line">    in_channels_stage2 = backbone.inplanes // <span class="number">8</span></span><br><span class="line">    in_channels_list = [in_channels_stage2 * <span class="number">2</span> ** (i - <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> returned_layers]</span><br><span class="line">    out_channels = <span class="number">256</span></span><br><span class="line">    <span class="keyword">return</span> BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels, extra_blocks=extra_blocks)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>首先，根据传入参数选出对应的resnet模型进行实例化。</p>
<p>随后，检查<code>trainable_layers</code>参数的合法值范围，并通过<code>parameter.requires_grad_(False)</code>来freeze除此以外的其他层。</p>
<p>默认未定义<code>extra_blocks</code>的时候，会在feature
map结尾添加一个maxpool2d层，该<code>LastLevelMaxPool</code>类实现并不复杂：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># defined in torchvision.ops.feature_pyramid_network</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LastLevelMaxPool</span>(<span class="title class_ inherited__">ExtraFPNBlock</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Applies a max_pool2d on top of the last feature map</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: <span class="type">List</span>[Tensor],</span></span><br><span class="line"><span class="params">        y: <span class="type">List</span>[Tensor],</span></span><br><span class="line"><span class="params">        names: <span class="type">List</span>[<span class="built_in">str</span>],</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        names.append(<span class="string">&quot;pool&quot;</span>)</span><br><span class="line">        x.append(F.max_pool2d(x[-<span class="number">1</span>], <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> x, names</span><br></pre></td></tr></table></figure>
<p>根据官方文档<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.functional.html?highlight=max_pool2d#torch.nn.functional.max_pool2d">torch.nn.functional.max_pool2d</a>可进一步查阅<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">torch.nn.MaxPool2d</a>，实际上<code>F.max_pool2d(x[-1], 1, 2, 0)</code>表示：</p>
<ol type="1">
<li>输入input为<code>x[-1]</code>；</li>
<li>池化窗口大小kernel_size为1；</li>
<li>步长stride为2；</li>
<li>边界填充padding为0。</li>
</ol>
<p>关于卷积类的操作可以结合可视化理解：</p>
<blockquote>
<p><a
target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">Convolution
arithmetic</a></p>
</blockquote>
<p>然后，处理其他传参：</p>
<ol type="1">
<li><code>return_layers</code>，这是一个dict，与传入的backbone相配合，key是backbone的module
name，value是用户定义的返回名；</li>
<li><code>in_channels_list</code>，这是一个list，与传入的backbone和<code>return_layers</code>相配合，是backbone返回的每一层feature
map的通道数；</li>
<li><code>out_channels</code>，一个整数，FPN中的通道数。</li>
</ol>
<h2 id="实现层">2 实现层</h2>
<p>我们以一个例子贯穿始终：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">x = [torch.rand(<span class="number">3</span>, <span class="number">300</span>, <span class="number">400</span>), torch.rand(<span class="number">3</span>, <span class="number">500</span>, <span class="number">400</span>)]	<span class="comment"># 模拟输入两张尺寸不同的图片</span></span><br><span class="line">predictions = model(x)</span><br></pre></td></tr></table></figure>
<p>我们使用预训练模型，并模拟输入两张图片。均为3通道，一张<span
class="math inline">\(300 \times 400\)</span>的<span
class="math inline">\(H \times W\)</span>分辨率，一张<span
class="math inline">\(500 \times 400\)</span>。</p>
<h3 id="fasterrcnn">FasterRCNN</h3>
<p><code>FasterRCNN</code>类在<code>torchvision.models.detection.faster_rcnn.py</code>中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FasterRCNN</span>(<span class="title class_ inherited__">GeneralizedRCNN</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements Faster R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each</span></span><br><span class="line"><span class="string">    image, and should be in 0-1 range. Different images can have different sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The behavior of the model changes depending if it is in training or evaluation mode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training, the model expects both the input tensors, as well as a targets (list of dictionary),</span></span><br><span class="line"><span class="string">    containing:</span></span><br><span class="line"><span class="string">        - boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values of x</span></span><br><span class="line"><span class="string">          between 0 and W and values of y between 0 and H</span></span><br><span class="line"><span class="string">        - labels (Int64Tensor[N]): the class label for each ground-truth box</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The model returns a Dict[Tensor] during training, containing the classification and regression</span></span><br><span class="line"><span class="string">    losses for both the RPN and the R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During inference, the model requires only the input tensors, and returns the post-processed</span></span><br><span class="line"><span class="string">    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as</span></span><br><span class="line"><span class="string">    follows:</span></span><br><span class="line"><span class="string">        - boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values of x</span></span><br><span class="line"><span class="string">          between 0 and W and values of y between 0 and H</span></span><br><span class="line"><span class="string">        - labels (Int64Tensor[N]): the predicted labels for each image</span></span><br><span class="line"><span class="string">        - scores (Tensor[N]): the scores or each prediction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        backbone (nn.Module): the network used to compute the features for the model.</span></span><br><span class="line"><span class="string">            It should contain a out_channels attribute, which indicates the number of output</span></span><br><span class="line"><span class="string">            channels that each feature map has (and it should be the same for all feature maps).</span></span><br><span class="line"><span class="string">            The backbone should return a single Tensor or and OrderedDict[Tensor].</span></span><br><span class="line"><span class="string">        num_classes (int): number of output classes of the model (including the background).</span></span><br><span class="line"><span class="string">            If box_predictor is specified, num_classes should be None.</span></span><br><span class="line"><span class="string">        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone</span></span><br><span class="line"><span class="string">        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone</span></span><br><span class="line"><span class="string">        image_mean (Tuple[float, float, float]): mean values used for input normalization.</span></span><br><span class="line"><span class="string">            They are generally the mean values of the dataset on which the backbone has been trained</span></span><br><span class="line"><span class="string">            on</span></span><br><span class="line"><span class="string">        image_std (Tuple[float, float, float]): std values used for input normalization.</span></span><br><span class="line"><span class="string">            They are generally the std values of the dataset on which the backbone has been trained on</span></span><br><span class="line"><span class="string">        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature</span></span><br><span class="line"><span class="string">            maps.</span></span><br><span class="line"><span class="string">        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN</span></span><br><span class="line"><span class="string">        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training</span></span><br><span class="line"><span class="string">        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing</span></span><br><span class="line"><span class="string">        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training</span></span><br><span class="line"><span class="string">        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing</span></span><br><span class="line"><span class="string">        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals</span></span><br><span class="line"><span class="string">        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as positive during training of the RPN.</span></span><br><span class="line"><span class="string">        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as negative during training of the RPN.</span></span><br><span class="line"><span class="string">        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN</span></span><br><span class="line"><span class="string">            for computing the loss</span></span><br><span class="line"><span class="string">        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training</span></span><br><span class="line"><span class="string">            of the RPN</span></span><br><span class="line"><span class="string">        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in</span></span><br><span class="line"><span class="string">            the locations indicated by the bounding boxes</span></span><br><span class="line"><span class="string">        box_head (nn.Module): module that takes the cropped feature maps as input</span></span><br><span class="line"><span class="string">        box_predictor (nn.Module): module that takes the output of box_head and returns the</span></span><br><span class="line"><span class="string">            classification logits and box regression deltas.</span></span><br><span class="line"><span class="string">        box_score_thresh (float): during inference, only return proposals with a classification score</span></span><br><span class="line"><span class="string">            greater than box_score_thresh</span></span><br><span class="line"><span class="string">        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference</span></span><br><span class="line"><span class="string">        box_detections_per_img (int): maximum number of detections per image, for all classes.</span></span><br><span class="line"><span class="string">        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as positive during training of the classification head</span></span><br><span class="line"><span class="string">        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as negative during training of the classification head</span></span><br><span class="line"><span class="string">        box_batch_size_per_image (int): number of proposals that are sampled during training of the</span></span><br><span class="line"><span class="string">            classification head</span></span><br><span class="line"><span class="string">        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training</span></span><br><span class="line"><span class="string">            of the classification head</span></span><br><span class="line"><span class="string">        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the</span></span><br><span class="line"><span class="string">            bounding boxes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; import torch</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; import torchvision</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; from torchvision.models.detection import FasterRCNN</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; from torchvision.models.detection.rpn import AnchorGenerator</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # load a pre-trained model for classification and return</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # only the features</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; backbone = torchvision.models.mobilenet_v2(pretrained=True).features</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # FasterRCNN needs to know the number of</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # output channels in a backbone. For mobilenet_v2, it&#x27;s 1280</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # so we need to add it here</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; backbone.out_channels = 1280</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # let&#x27;s make the RPN generate 5 x 3 anchors per spatial</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # location, with 5 different sizes and 3 different aspect</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # ratios. We have a Tuple[Tuple[int]] because each feature</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # map could potentially have different sizes and</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # aspect ratios</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;                                    aspect_ratios=((0.5, 1.0, 2.0),))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # let&#x27;s define what are the feature maps that we will</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # use to perform the region of interest cropping, as well as</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # the size of the crop after rescaling.</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # if your backbone returns a Tensor, featmap_names is expected to</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # be [&#x27;0&#x27;]. More generally, the backbone should return an</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # OrderedDict[Tensor], and in featmap_names you can choose which</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # feature maps to use.</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[&#x27;0&#x27;],</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;                                                 output_size=7,</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;                                                 sampling_ratio=2)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # put the pieces together inside a FasterRCNN model</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = FasterRCNN(backbone,</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;                    num_classes=2,</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;                    rpn_anchor_generator=anchor_generator,</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;                    box_roi_pool=roi_pooler)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model.eval()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; predictions = model(x)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone, num_classes=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># transform parameters</span></span></span><br><span class="line"><span class="params">                 min_size=<span class="number">800</span>, max_size=<span class="number">1333</span>,</span></span><br><span class="line"><span class="params">                 image_mean=<span class="literal">None</span>, image_std=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># RPN parameters</span></span></span><br><span class="line"><span class="params">                 rpn_anchor_generator=<span class="literal">None</span>, rpn_head=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 rpn_pre_nms_top_n_train=<span class="number">2000</span>, rpn_pre_nms_top_n_test=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 rpn_post_nms_top_n_train=<span class="number">2000</span>, rpn_post_nms_top_n_test=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 rpn_nms_thresh=<span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">                 rpn_fg_iou_thresh=<span class="number">0.7</span>, rpn_bg_iou_thresh=<span class="number">0.3</span>,</span></span><br><span class="line"><span class="params">                 rpn_batch_size_per_image=<span class="number">256</span>, rpn_positive_fraction=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># Box parameters</span></span></span><br><span class="line"><span class="params">                 box_roi_pool=<span class="literal">None</span>, box_head=<span class="literal">None</span>, box_predictor=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 box_score_thresh=<span class="number">0.05</span>, box_nms_thresh=<span class="number">0.5</span>, box_detections_per_img=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">                 box_fg_iou_thresh=<span class="number">0.5</span>, box_bg_iou_thresh=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 box_batch_size_per_image=<span class="number">512</span>, box_positive_fraction=<span class="number">0.25</span>,</span></span><br><span class="line"><span class="params">                 bbox_reg_weights=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(backbone, <span class="string">&quot;out_channels&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;backbone should contain an attribute out_channels &quot;</span></span><br><span class="line">                <span class="string">&quot;specifying the number of output channels (assumed to be the &quot;</span></span><br><span class="line">                <span class="string">&quot;same for all the levels)&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(rpn_anchor_generator, (AnchorGenerator, <span class="built_in">type</span>(<span class="literal">None</span>)))</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(box_roi_pool, (MultiScaleRoIAlign, <span class="built_in">type</span>(<span class="literal">None</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;num_classes should be None when box_predictor is specified&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;num_classes should not be None when box_predictor &quot;</span></span><br><span class="line">                                 <span class="string">&quot;is not specified&quot;</span>)</span><br><span class="line"></span><br><span class="line">        out_channels = backbone.out_channels</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rpn_anchor_generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            anchor_sizes = ((<span class="number">32</span>,), (<span class="number">64</span>,), (<span class="number">128</span>,), (<span class="number">256</span>,), (<span class="number">512</span>,))</span><br><span class="line">            aspect_ratios = ((<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>),) * <span class="built_in">len</span>(anchor_sizes)</span><br><span class="line">            rpn_anchor_generator = AnchorGenerator(</span><br><span class="line">                anchor_sizes, aspect_ratios</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">if</span> rpn_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            rpn_head = RPNHead(</span><br><span class="line">                out_channels, rpn_anchor_generator.num_anchors_per_location()[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        rpn_pre_nms_top_n = <span class="built_in">dict</span>(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)</span><br><span class="line">        rpn_post_nms_top_n = <span class="built_in">dict</span>(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)</span><br><span class="line"></span><br><span class="line">        rpn = RegionProposalNetwork(</span><br><span class="line">            rpn_anchor_generator, rpn_head,</span><br><span class="line">            rpn_fg_iou_thresh, rpn_bg_iou_thresh,</span><br><span class="line">            rpn_batch_size_per_image, rpn_positive_fraction,</span><br><span class="line">            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_roi_pool <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            box_roi_pool = MultiScaleRoIAlign(</span><br><span class="line">                featmap_names=[<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>],</span><br><span class="line">                output_size=<span class="number">7</span>,</span><br><span class="line">                sampling_ratio=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            resolution = box_roi_pool.output_size[<span class="number">0</span>]</span><br><span class="line">            representation_size = <span class="number">1024</span></span><br><span class="line">            box_head = TwoMLPHead(</span><br><span class="line">                out_channels * resolution ** <span class="number">2</span>,</span><br><span class="line">                representation_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            representation_size = <span class="number">1024</span></span><br><span class="line">            box_predictor = FastRCNNPredictor(</span><br><span class="line">                representation_size,</span><br><span class="line">                num_classes)</span><br><span class="line"></span><br><span class="line">        roi_heads = RoIHeads(</span><br><span class="line">            <span class="comment"># Box</span></span><br><span class="line">            box_roi_pool, box_head, box_predictor,</span><br><span class="line">            box_fg_iou_thresh, box_bg_iou_thresh,</span><br><span class="line">            box_batch_size_per_image, box_positive_fraction,</span><br><span class="line">            bbox_reg_weights,</span><br><span class="line">            box_score_thresh, box_nms_thresh, box_detections_per_img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> image_mean <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            image_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">        <span class="keyword">if</span> image_std <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            image_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>FasterRCNN</code>的代码看起来很长，实际上主要是文档注释。</p>
<p><code>FasterRCNN</code>的实现只有<code>__init__</code>函数，因为<code>FasterRCNN</code>继承自<code>GeneralizedRCNN</code>，主要结构和计算流的实现都在父类中实现了，该子类的实现实际上只需要做一些参数检查和子类的具体子结构的实例化。</p>
<p>FasterRCNN的<code>__init__</code>函数的主要就是在做参数检查和一些实例化准备工作，其结果就是将准备好的backbone、rpn、roi_heads和transform对象传递给父类（GeneralizedRCNN）的初始化函数，由此构建一个FasterRCNN实例对象。</p>
<h3 id="generalizedrcnn">GeneralizedRCNN</h3>
<p><code>GeneralizedRCNN</code>在<code>torchvision.models.detection.generalized_rcnn.py</code>中实现，负责以父类的形式定义RCNN架构的整体计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GeneralizedRCNN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Main class for Generalized R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        backbone (nn.Module):</span></span><br><span class="line"><span class="string">        rpn (nn.Module):</span></span><br><span class="line"><span class="string">        roi_heads (nn.Module): takes the features + the proposals from the RPN and computes</span></span><br><span class="line"><span class="string">            detections / masks from it.</span></span><br><span class="line"><span class="string">        transform (nn.Module): performs the data transformation from the inputs to feed into</span></span><br><span class="line"><span class="string">            the model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone, rpn, roi_heads, transform</span>):</span><br><span class="line">        <span class="built_in">super</span>(GeneralizedRCNN, self).__init__()</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.backbone = backbone</span><br><span class="line">        self.rpn = rpn</span><br><span class="line">        self.roi_heads = roi_heads</span><br><span class="line">        <span class="comment"># used only on torchscript mode</span></span><br><span class="line">        self._has_warned = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.jit.unused</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eager_outputs</span>(<span class="params">self, losses, detections</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor], <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]) -&gt; <span class="type">Union</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor], <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]]</span></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> losses</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> detections</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, images, targets=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]]) -&gt; <span class="type">Tuple</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor], <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            images (list[Tensor]): images to be processed</span></span><br><span class="line"><span class="string">            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            result (list[BoxList] or dict[Tensor]): the output from the model.</span></span><br><span class="line"><span class="string">                During training, it returns a dict[Tensor] which contains the losses.</span></span><br><span class="line"><span class="string">                During testing, it returns list[BoxList] contains additional fields</span></span><br><span class="line"><span class="string">                like `scores`, `labels` and `mask` (for Mask R-CNN models).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;In training mode, targets should be passed&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> target <span class="keyword">in</span> targets:</span><br><span class="line">                boxes = target[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(boxes, torch.Tensor):</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(boxes.shape) != <span class="number">2</span> <span class="keyword">or</span> boxes.shape[-<span class="number">1</span>] != <span class="number">4</span>:</span><br><span class="line">                        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected target boxes to be a tensor&quot;</span></span><br><span class="line">                                         <span class="string">&quot;of shape [N, 4], got &#123;:&#125;.&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                                             boxes.shape))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Expected target boxes to be of type &quot;</span></span><br><span class="line">                                     <span class="string">&quot;Tensor, got &#123;:&#125;.&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(boxes)))</span><br><span class="line"></span><br><span class="line">        original_image_sizes = torch.jit.annotate(<span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]], [])</span><br><span class="line">        <span class="keyword">for</span> img <span class="keyword">in</span> images:</span><br><span class="line">            val = img.shape[-<span class="number">2</span>:]</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(val) == <span class="number">2</span></span><br><span class="line">            original_image_sizes.append((val[<span class="number">0</span>], val[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        images, targets = self.transform(images, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for degenerate boxes</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Move this to a function</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">for</span> target_idx, target <span class="keyword">in</span> <span class="built_in">enumerate</span>(targets):</span><br><span class="line">                boxes = target[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line">                degenerate_boxes = boxes[:, <span class="number">2</span>:] &lt;= boxes[:, :<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">if</span> degenerate_boxes.<span class="built_in">any</span>():</span><br><span class="line">                    <span class="comment"># print the first degenerate box</span></span><br><span class="line">                    bb_idx = torch.where(degenerate_boxes.<span class="built_in">any</span>(dim=<span class="number">1</span>))[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">                    degen_bb: <span class="type">List</span>[<span class="built_in">float</span>] = boxes[bb_idx].tolist()</span><br><span class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">&quot;All bounding boxes should have positive height and width.&quot;</span></span><br><span class="line">                                     <span class="string">&quot; Found invalid box &#123;&#125; for target at index &#123;&#125;.&quot;</span></span><br><span class="line">                                     .<span class="built_in">format</span>(degen_bb, target_idx))</span><br><span class="line"></span><br><span class="line">        features = self.backbone(images.tensors)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(features, torch.Tensor):</span><br><span class="line">            features = OrderedDict([(<span class="string">&#x27;0&#x27;</span>, features)])</span><br><span class="line">        proposals, proposal_losses = self.rpn(images, features, targets)</span><br><span class="line">        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)</span><br><span class="line">        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)</span><br><span class="line"></span><br><span class="line">        losses = &#123;&#125;</span><br><span class="line">        losses.update(detector_losses)</span><br><span class="line">        losses.update(proposal_losses)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.jit.is_scripting():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._has_warned:</span><br><span class="line">                warnings.warn(<span class="string">&quot;RCNN always returns a (Losses, Detections) tuple in scripting&quot;</span>)</span><br><span class="line">                self._has_warned = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> (losses, detections)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.eager_outputs(losses, detections)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在<code>__init__</code>中，GenerailizedRCNN把R-CNN架构定义为4个组成部分：</p>
<ol type="1">
<li><code>transform</code>：一个变换模型，用于对图像和其他输入进行变换；</li>
<li><code>backbone</code>：一个特征提取模型，输入的是进过变换处理的图像张量，输出的是取得的图像特征features；</li>
<li><code>rpn</code>：一个RPN模型，输入包含——图像images、backbone提取出的图像特征features以及训练时输入的包含bbox
ground truth的targets，输出包含——预测的区域proposals和相应的损失；</li>
<li><code>roi_heads</code>：一个RoIHeads模型，输入包含——backbone输出的features，RPN输出的proposals，以及图像尺寸和训练时的targets。</li>
</ol>
<p>该类的<code>__forward__</code>计算流差不多就是这四部分依次执行的过程，除了一些参数检查，训练时和预测时对输入的区分以外，主要代码逻辑可以概括为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, images, targets=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 1. transform</span></span><br><span class="line">    images, targets = self.transform(images, targets)</span><br><span class="line">    <span class="comment"># 2. backbone</span></span><br><span class="line">    features = self.backbone(images.tensors)</span><br><span class="line">    <span class="comment"># 3. rpn</span></span><br><span class="line">    proposals, proposal_losses = self.rpn(images, features, targets)</span><br><span class="line">    <span class="comment"># 4. roi_heads</span></span><br><span class="line">    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)</span><br><span class="line">    <span class="comment"># 5. postprocess</span></span><br><span class="line">    detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)</span><br></pre></td></tr></table></figure>
<h4 id="generalizedrcnntransform">GeneralizedRCNNTransform</h4>
<p>Faster
R-CNN模型对输入图像的预处理由<code>torchvision.models.detection.transform</code>包的<code>GeneralizedRCNNTransform</code>类实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GeneralizedRCNNTransform</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs input / target transformation before feeding the data to a GeneralizedRCNN</span></span><br><span class="line"><span class="string">    model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The transformations it perform are:</span></span><br><span class="line"><span class="string">        - input normalization (mean subtraction and std division)</span></span><br><span class="line"><span class="string">        - input / target resizing to match min_size / max_size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, min_size, max_size, image_mean, image_std</span>):</span><br><span class="line">        <span class="built_in">super</span>(GeneralizedRCNNTransform, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(min_size, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            min_size = (min_size,)</span><br><span class="line">        self.min_size = min_size</span><br><span class="line">        self.max_size = max_size</span><br><span class="line">        self.image_mean = image_mean</span><br><span class="line">        self.image_std = image_std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                images,       <span class="comment"># type: List[Tensor]</span></span></span><br><span class="line"><span class="params">                targets=<span class="literal">None</span>  <span class="comment"># type: Optional[List[Dict[str, Tensor]]]</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">Tuple</span>[ImageList, <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]]]</span></span><br><span class="line">        images = [img <span class="keyword">for</span> img <span class="keyword">in</span> images]</span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># make a copy of targets to avoid modifying it in-place</span></span><br><span class="line">            <span class="comment"># once torchscript supports dict comprehension</span></span><br><span class="line">            <span class="comment"># this can be simplified as as follows</span></span><br><span class="line">            <span class="comment"># targets = [&#123;k: v for k,v in t.items()&#125; for t in targets]</span></span><br><span class="line">            targets_copy: <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]] = []</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">                data: <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor] = &#123;&#125;</span><br><span class="line">                <span class="keyword">for</span> k, v <span class="keyword">in</span> t.items():</span><br><span class="line">                    data[k] = v</span><br><span class="line">                targets_copy.append(data)</span><br><span class="line">            targets = targets_copy</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(images)):</span><br><span class="line">            image = images[i]</span><br><span class="line">            target_index = targets[i] <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> image.dim() != <span class="number">3</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;images is expected to be a list of 3d tensors &quot;</span></span><br><span class="line">                                 <span class="string">&quot;of shape [C, H, W], got &#123;&#125;&quot;</span>.<span class="built_in">format</span>(image.shape))</span><br><span class="line">            image = self.normalize(image)</span><br><span class="line">            image, target_index = self.resize(image, target_index)</span><br><span class="line">            images[i] = image</span><br><span class="line">            <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> target_index <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                targets[i] = target_index</span><br><span class="line"></span><br><span class="line">        image_sizes = [img.shape[-<span class="number">2</span>:] <span class="keyword">for</span> img <span class="keyword">in</span> images]</span><br><span class="line">        images = self.batch_images(images)</span><br><span class="line">        image_sizes_list = torch.jit.annotate(<span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]], [])</span><br><span class="line">        <span class="keyword">for</span> image_size <span class="keyword">in</span> image_sizes:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(image_size) == <span class="number">2</span></span><br><span class="line">            image_sizes_list.append((image_size[<span class="number">0</span>], image_size[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        image_list = ImageList(images, image_sizes_list)</span><br><span class="line">        <span class="keyword">return</span> image_list, targets</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">self, image</span>):</span><br><span class="line">        dtype, device = image.dtype, image.device</span><br><span class="line">        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)</span><br><span class="line">        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)</span><br><span class="line">        <span class="keyword">return</span> (image - mean[:, <span class="literal">None</span>, <span class="literal">None</span>]) / std[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">torch_choice</span>(<span class="params">self, k</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="built_in">int</span>]) -&gt; <span class="built_in">int</span></span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Implements `random.choice` via torch ops so it can be compiled with</span></span><br><span class="line"><span class="string">        TorchScript. Remove if https://github.com/pytorch/pytorch/issues/25803</span></span><br><span class="line"><span class="string">        is fixed.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        index = <span class="built_in">int</span>(torch.empty(<span class="number">1</span>).uniform_(<span class="number">0.</span>, <span class="built_in">float</span>(<span class="built_in">len</span>(k))).item())</span><br><span class="line">        <span class="keyword">return</span> k[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">resize</span>(<span class="params">self, image, target</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, <span class="type">Optional</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]]</span></span><br><span class="line">        h, w = image.shape[-<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            size = <span class="built_in">float</span>(self.torch_choice(self.min_size))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># FIXME assume for now that testing uses the largest scale</span></span><br><span class="line">            size = <span class="built_in">float</span>(self.min_size[-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">            image, target = _resize_image_and_masks_onnx(image, size, <span class="built_in">float</span>(self.max_size), target)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image, target = _resize_image_and_masks(image, size, <span class="built_in">float</span>(self.max_size), target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> image, target</span><br><span class="line"></span><br><span class="line">        bbox = target[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line">        bbox = resize_boxes(bbox, (h, w), image.shape[-<span class="number">2</span>:])</span><br><span class="line">        target[<span class="string">&quot;boxes&quot;</span>] = bbox</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;keypoints&quot;</span> <span class="keyword">in</span> target:</span><br><span class="line">            keypoints = target[<span class="string">&quot;keypoints&quot;</span>]</span><br><span class="line">            keypoints = resize_keypoints(keypoints, (h, w), image.shape[-<span class="number">2</span>:])</span><br><span class="line">            target[<span class="string">&quot;keypoints&quot;</span>] = keypoints</span><br><span class="line">        <span class="keyword">return</span> image, target</span><br><span class="line"></span><br><span class="line">    <span class="comment"># _onnx_batch_images() is an implementation of</span></span><br><span class="line">    <span class="comment"># batch_images() that is supported by ONNX tracing.</span></span><br><span class="line"><span class="meta">    @torch.jit.unused</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_onnx_batch_images</span>(<span class="params">self, images, size_divisible=<span class="number">32</span></span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="built_in">int</span>) -&gt; Tensor</span></span><br><span class="line">        max_size = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(images[<span class="number">0</span>].dim()):</span><br><span class="line">            max_size_i = torch.<span class="built_in">max</span>(torch.stack([img.shape[i] <span class="keyword">for</span> img <span class="keyword">in</span> images]).to(torch.float32)).to(torch.int64)</span><br><span class="line">            max_size.append(max_size_i)</span><br><span class="line">        stride = size_divisible</span><br><span class="line">        max_size[<span class="number">1</span>] = (torch.ceil((max_size[<span class="number">1</span>].to(torch.float32)) / stride) * stride).to(torch.int64)</span><br><span class="line">        max_size[<span class="number">2</span>] = (torch.ceil((max_size[<span class="number">2</span>].to(torch.float32)) / stride) * stride).to(torch.int64)</span><br><span class="line">        max_size = <span class="built_in">tuple</span>(max_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># work around for</span></span><br><span class="line">        <span class="comment"># pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)</span></span><br><span class="line">        <span class="comment"># which is not yet supported in onnx</span></span><br><span class="line">        padded_imgs = []</span><br><span class="line">        <span class="keyword">for</span> img <span class="keyword">in</span> images:</span><br><span class="line">            padding = [(s1 - s2) <span class="keyword">for</span> s1, s2 <span class="keyword">in</span> <span class="built_in">zip</span>(max_size, <span class="built_in">tuple</span>(img.shape))]</span><br><span class="line">            padded_img = torch.nn.functional.pad(img, (<span class="number">0</span>, padding[<span class="number">2</span>], <span class="number">0</span>, padding[<span class="number">1</span>], <span class="number">0</span>, padding[<span class="number">0</span>]))</span><br><span class="line">            padded_imgs.append(padded_img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.stack(padded_imgs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">max_by_axis</span>(<span class="params">self, the_list</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]</span></span><br><span class="line">        maxes = the_list[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> sublist <span class="keyword">in</span> the_list[<span class="number">1</span>:]:</span><br><span class="line">            <span class="keyword">for</span> index, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(sublist):</span><br><span class="line">                maxes[index] = <span class="built_in">max</span>(maxes[index], item)</span><br><span class="line">        <span class="keyword">return</span> maxes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">batch_images</span>(<span class="params">self, images, size_divisible=<span class="number">32</span></span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="built_in">int</span>) -&gt; Tensor</span></span><br><span class="line">        <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">            <span class="comment"># batch_images() does not export well to ONNX</span></span><br><span class="line">            <span class="comment"># call _onnx_batch_images() instead</span></span><br><span class="line">            <span class="keyword">return</span> self._onnx_batch_images(images, size_divisible)</span><br><span class="line"></span><br><span class="line">        max_size = self.max_by_axis([<span class="built_in">list</span>(img.shape) <span class="keyword">for</span> img <span class="keyword">in</span> images])</span><br><span class="line">        stride = <span class="built_in">float</span>(size_divisible)</span><br><span class="line">        max_size = <span class="built_in">list</span>(max_size)</span><br><span class="line">        max_size[<span class="number">1</span>] = <span class="built_in">int</span>(math.ceil(<span class="built_in">float</span>(max_size[<span class="number">1</span>]) / stride) * stride)</span><br><span class="line">        max_size[<span class="number">2</span>] = <span class="built_in">int</span>(math.ceil(<span class="built_in">float</span>(max_size[<span class="number">2</span>]) / stride) * stride)</span><br><span class="line"></span><br><span class="line">        batch_shape = [<span class="built_in">len</span>(images)] + max_size</span><br><span class="line">        batched_imgs = images[<span class="number">0</span>].new_full(batch_shape, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> img, pad_img <span class="keyword">in</span> <span class="built_in">zip</span>(images, batched_imgs):</span><br><span class="line">            pad_img[: img.shape[<span class="number">0</span>], : img.shape[<span class="number">1</span>], : img.shape[<span class="number">2</span>]].copy_(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batched_imgs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                    result,               <span class="comment"># type: List[Dict[str, Tensor]]</span></span></span><br><span class="line"><span class="params">                    image_shapes,         <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                    original_image_sizes  <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                    </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]</span></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">for</span> i, (pred, im_s, o_im_s) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(result, image_shapes, original_image_sizes)):</span><br><span class="line">            boxes = pred[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line">            boxes = resize_boxes(boxes, im_s, o_im_s)</span><br><span class="line">            result[i][<span class="string">&quot;boxes&quot;</span>] = boxes</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;masks&quot;</span> <span class="keyword">in</span> pred:</span><br><span class="line">                masks = pred[<span class="string">&quot;masks&quot;</span>]</span><br><span class="line">                masks = paste_masks_in_image(masks, boxes, o_im_s)</span><br><span class="line">                result[i][<span class="string">&quot;masks&quot;</span>] = masks</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;keypoints&quot;</span> <span class="keyword">in</span> pred:</span><br><span class="line">                keypoints = pred[<span class="string">&quot;keypoints&quot;</span>]</span><br><span class="line">                keypoints = resize_keypoints(keypoints, im_s, o_im_s)</span><br><span class="line">                result[i][<span class="string">&quot;keypoints&quot;</span>] = keypoints</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        format_string = self.__class__.__name__ + <span class="string">&#x27;(&#x27;</span></span><br><span class="line">        _indent = <span class="string">&#x27;\n    &#x27;</span></span><br><span class="line">        format_string += <span class="string">&quot;&#123;0&#125;Normalize(mean=&#123;1&#125;, std=&#123;2&#125;)&quot;</span>.<span class="built_in">format</span>(_indent, self.image_mean, self.image_std)</span><br><span class="line">        format_string += <span class="string">&quot;&#123;0&#125;Resize(min_size=&#123;1&#125;, max_size=&#123;2&#125;, mode=&#x27;bilinear&#x27;)&quot;</span>.<span class="built_in">format</span>(_indent, self.min_size,</span><br><span class="line">                                                                                         self.max_size)</span><br><span class="line">        format_string += <span class="string">&#x27;\n)&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> format_string</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对输入图像的初步转换处理在<code>forward</code>前向传播函数中实现，主要实现normalize和resize操作：</p>
<ol type="1">
<li><code>self.normalize</code>：初始参数在FasterRCNN的初始化中被设为<code>image_mean = [0.485, 0.456, 0.406]</code>和<code>image_std = [0.229, 0.224, 0.225]</code>；</li>
<li><code>self.resize</code>：初始参数在FasterRCNN的初始化中被设为<code>min_size=800, max_size=1333</code>；</li>
<li><code>self.batch_images</code>，对一个batch的图像做了Padding，使其输出的张量尺寸一致。</li>
</ol>
<p>根据该转换模块的默认值，结合本节开头的例子：</p>
<ol type="1">
<li>经过resize处理后，因为最小尺寸必须为800，因此<span
class="math inline">\(300 \times 400\)</span>的图片1转换为了<span
class="math inline">\(800 \times 1066\)</span>，<span
class="math inline">\(400 \times 500\)</span>的图片2转换为了<span
class="math inline">\(1000 \times 800\)</span>；</li>
<li>因为batch处理转tensors时加padding的缘故，两个图片的张量尺寸被统一为<span
class="math inline">\(1024 \times 1088\)</span>。</li>
</ol>
<h4 id="backbonewithfpn">BackboneWithFPN</h4>
<p><code>BackboneWithFPN</code>在<code>torchvision.models.detection.backbone_utils</code>中实现，其作用就是以ResNet模型中提取出的一些中间层作为backbone，在backbone后面继续接上一个FPN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BackboneWithFPN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Adds a FPN on top of a model.</span></span><br><span class="line"><span class="string">    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to</span></span><br><span class="line"><span class="string">    extract a submodel that returns the feature maps specified in return_layers.</span></span><br><span class="line"><span class="string">    The same limitations of IntermediatLayerGetter apply here.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        backbone (nn.Module)</span></span><br><span class="line"><span class="string">        return_layers (Dict[name, new_name]): a dict containing the names</span></span><br><span class="line"><span class="string">            of the modules for which the activations will be returned as</span></span><br><span class="line"><span class="string">            the key of the dict, and the value of the dict is the name</span></span><br><span class="line"><span class="string">            of the returned activation (which the user can specify).</span></span><br><span class="line"><span class="string">        in_channels_list (List[int]): number of channels for each feature map</span></span><br><span class="line"><span class="string">            that is returned, in the order they are present in the OrderedDict</span></span><br><span class="line"><span class="string">        out_channels (int): number of channels in the FPN.</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        out_channels (int): the number of channels in the FPN</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone, return_layers, in_channels_list, out_channels, extra_blocks=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BackboneWithFPN, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> extra_blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            extra_blocks = LastLevelMaxPool()</span><br><span class="line"></span><br><span class="line">        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)</span><br><span class="line">        self.fpn = FeaturePyramidNetwork(</span><br><span class="line">            in_channels_list=in_channels_list,</span><br><span class="line">            out_channels=out_channels,</span><br><span class="line">            extra_blocks=extra_blocks,</span><br><span class="line">        )</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.body(x)</span><br><span class="line">        x = self.fpn(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>该类的实现很简单，就像是一个组合，把backbone和FPN装起来：</p>
<ol type="1">
<li>把从backbone中取出的（用于提供feature
maps）中间层作为模型的body；</li>
<li>构造出FPN（FeaturePyramidNetworkj）作为模型的fpn；</li>
</ol>
<p>然后数据流定义很简洁，就是输入数据x先经过body，再经过fpn，就完成了。</p>
<p>有了backbone+FPN的模型，就可以进一步构造Faster R-CNN模型了。</p>
<p><code>torchvision</code>的Faster
R-CNN的backbone负责提取图像特征，具体实现由ResNet中间层衔接FPN组成。</p>
<h5 id="resnet">ResNet</h5>
<p><code>class ResNet(nn.Module)</code></p>
<p>ResNet在<code>torchvision.models.resnet</code>包中实现，属于卷积神经网络实现的范畴，本文不再赘述。</p>
<p>有了resnet作为backbone，就可以通过<code>resnet_fpn_backbone</code>构造一个在resnet后面接上FPN的模型，具体地，是构造<code>BackboneWithFPN</code>类的对象。</p>
<p><code>torchvision</code>实现中：</p>
<ol type="1">
<li>默认后三层，即ResNet的layer4, layer3,
layer2为可训练层，其余freeze；</li>
<li>默认返回后四层的feature map，即layer1, layer2, layer3,
layer4，命名index依次为0, 1, 2, 3，每层输出feature map的通道数依次为256,
512, 1024, 2048。</li>
</ol>
<p>本节的例子经过ResNet部分的计算后，从输入的<span
class="math inline">\(2 \times 3 \times 1024 \times
1088\)</span>的tensor，转换为了一个有序字典OrderedDict：</p>
<ol type="1">
<li><code>'0': shape[2, 256, 256, 272]</code>，源自ResNet的layer1；</li>
<li><code>'1': shape[2, 512, 128, 136]</code>，源自ResNet的layer2；</li>
<li><code>'2': shape[2, 1024, 64, 68]</code>，源自ResNet的layer3；</li>
<li><code>'3': shape[2, 2048, 32, 34]</code>，源自ResNet的layer4；</li>
</ol>
<h5 id="fpnfeaturepyramidnetwork">FPN(FeaturePyramidNetwork)</h5>
<p><code>FeaturePyramidNetwork</code>在<code>torchvision.ops.feature_pyramid_network</code>包中实现。FPN实现了金字塔结构的特征提取，低层的卷积感受野小，其特征代表小目标的特征，而高层的卷积感受野大，因此其特征适合表示大目标特征。在目标检测中运用FPN，在低层配合小尺寸anchor，在高层配合大尺寸anchors，有利于同时有效检测小目标和大目标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeaturePyramidNetwork</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Module that adds a FPN from on top of a set of feature maps. This is based on</span></span><br><span class="line"><span class="string">    `&quot;Feature Pyramid Network for Object Detection&quot; &lt;https://arxiv.org/abs/1612.03144&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The feature maps are currently supposed to be in increasing depth</span></span><br><span class="line"><span class="string">    order.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is expected to be an OrderedDict[Tensor], containing</span></span><br><span class="line"><span class="string">    the feature maps on top of which the FPN will be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels_list (list[int]): number of channels for each feature map that</span></span><br><span class="line"><span class="string">            is passed to the module</span></span><br><span class="line"><span class="string">        out_channels (int): number of channels of the FPN representation</span></span><br><span class="line"><span class="string">        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will</span></span><br><span class="line"><span class="string">            be performed. It is expected to take the fpn features, the original</span></span><br><span class="line"><span class="string">            features and the names of the original features as input, and returns</span></span><br><span class="line"><span class="string">            a new list of feature maps and their corresponding names</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # get some dummy data</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x = OrderedDict()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x[&#x27;feat0&#x27;] = torch.rand(1, 10, 64, 64)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x[&#x27;feat2&#x27;] = torch.rand(1, 20, 16, 16)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x[&#x27;feat3&#x27;] = torch.rand(1, 30, 8, 8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # compute the FPN on top of x</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = m(x)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print([(k, v.shape) for k, v in output.items()])</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # returns</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;   [(&#x27;feat0&#x27;, torch.Size([1, 5, 64, 64])),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;    (&#x27;feat2&#x27;, torch.Size([1, 5, 16, 16])),</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;    (&#x27;feat3&#x27;, torch.Size([1, 5, 8, 8]))]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channels_list: <span class="type">List</span>[<span class="built_in">int</span>],</span></span><br><span class="line"><span class="params">        out_channels: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        extra_blocks: <span class="type">Optional</span>[ExtraFPNBlock] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(FeaturePyramidNetwork, self).__init__()</span><br><span class="line">        self.inner_blocks = nn.ModuleList()</span><br><span class="line">        self.layer_blocks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> in_channels <span class="keyword">in</span> in_channels_list:</span><br><span class="line">            <span class="keyword">if</span> in_channels == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;in_channels=0 is currently not supported&quot;</span>)</span><br><span class="line">            inner_block_module = nn.Conv2d(in_channels, out_channels, <span class="number">1</span>)</span><br><span class="line">            layer_block_module = nn.Conv2d(out_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            self.inner_blocks.append(inner_block_module)</span><br><span class="line">            self.layer_blocks.append(layer_block_module)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize parameters now to avoid modifying the initialization of top_blocks</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.children():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_uniform_(m.weight, a=<span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> extra_blocks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">isinstance</span>(extra_blocks, ExtraFPNBlock)</span><br><span class="line">        self.extra_blocks = extra_blocks</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_result_from_inner_blocks</span>(<span class="params">self, x: Tensor, idx: <span class="built_in">int</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This is equivalent to self.inner_blocks[idx](x),</span></span><br><span class="line"><span class="string">        but torchscript doesn&#x27;t support this yet</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_blocks = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.inner_blocks:</span><br><span class="line">            num_blocks += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> idx &lt; <span class="number">0</span>:</span><br><span class="line">            idx += num_blocks</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.inner_blocks:</span><br><span class="line">            <span class="keyword">if</span> i == idx:</span><br><span class="line">                out = module(x)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_result_from_layer_blocks</span>(<span class="params">self, x: Tensor, idx: <span class="built_in">int</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This is equivalent to self.layer_blocks[idx](x),</span></span><br><span class="line"><span class="string">        but torchscript doesn&#x27;t support this yet</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_blocks = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.layer_blocks:</span><br><span class="line">            num_blocks += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> idx &lt; <span class="number">0</span>:</span><br><span class="line">            idx += num_blocks</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.layer_blocks:</span><br><span class="line">            <span class="keyword">if</span> i == idx:</span><br><span class="line">                out = module(x)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Computes the FPN for a set of feature maps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            x (OrderedDict[Tensor]): feature maps for each feature level.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            results (OrderedDict[Tensor]): feature maps after FPN layers.</span></span><br><span class="line"><span class="string">                They are ordered from highest resolution first.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># unpack OrderedDict into two lists for easier handling</span></span><br><span class="line">        names = <span class="built_in">list</span>(x.keys())</span><br><span class="line">        x = <span class="built_in">list</span>(x.values())</span><br><span class="line"></span><br><span class="line">        last_inner = self.get_result_from_inner_blocks(x[-<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        results = []</span><br><span class="line">        results.append(self.get_result_from_layer_blocks(last_inner, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x) - <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            inner_lateral = self.get_result_from_inner_blocks(x[idx], idx)</span><br><span class="line">            feat_shape = inner_lateral.shape[-<span class="number">2</span>:]</span><br><span class="line">            inner_top_down = F.interpolate(last_inner, size=feat_shape, mode=<span class="string">&quot;nearest&quot;</span>)</span><br><span class="line">            last_inner = inner_lateral + inner_top_down</span><br><span class="line">            results.insert(<span class="number">0</span>, self.get_result_from_layer_blocks(last_inner, idx))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.extra_blocks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            results, names = self.extra_blocks(results, x, names)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># make it back an OrderedDict</span></span><br><span class="line">        out = OrderedDict([(k, v) <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">zip</span>(names, results)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>按原论文的思路，FPN第n层输出feature map <span
class="math inline">\(P_n\)</span>的是把两者进行合并：</p>
<ol type="1">
<li>lateral：CNN的第n层feature map <span
class="math inline">\(C_n\)</span>，做1×1卷积；</li>
<li>top-down upsampling：FPN的n+1层feature map <span
class="math inline">\(P_{n+1}\)</span>做2×上采样（长宽各2倍）变成第n层的尺寸；</li>
</ol>
<p>此后，采用3×3卷积对合并后的feature
map进行卷积处理，以便消除上采样操作造成的失真效应（aliasing
effect）。</p>
<p>此时，形成的每层的最终的feature map就是最终的feature map <span
class="math inline">\(P_n\)</span>，例如：从ResNet的2~5层feature map
<span class="math inline">\(\{C_2, C_3, C_4,
C_5\}\)</span>经过FPN取得<span class="math inline">\(\{P_2, P_3, P_4,
P_5\}\)</span>，对应的两者的空域尺寸（spatial size）是相同的。</p>
<p>在<code>torchvision</code>的具体实现中：</p>
<ol type="1">
<li><code>self.inner_blocks</code>就是FPN的所有1×1卷积；</li>
<li><code>self.layer_blocks</code>就是FPN合并后需要用到的3×3卷积；</li>
</ol>
<p>这两者都是<code>nn.ModuleList()</code>，在<code>__init__</code>初始化时，在一个n次（n个feature
map）的for循环中进行初始化，都填入<code>nn.Conv2d</code>对象，设置为统一的<code>out_channels</code>。</p>
<p>在<code>__forward__</code>定义的计算流中，核心代码逻辑可以概括为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]</span>):</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x) - <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        inner_lateral = self.get_result_from_inner_blocks(x[idx], idx)</span><br><span class="line">        feat_shape = inner_lateral.shape[-<span class="number">2</span>:]</span><br><span class="line">        inner_top_down = F.interpolate(last_inner, size=feat_shape, mode=<span class="string">&quot;nearest&quot;</span>)</span><br><span class="line">        last_inner = inner_lateral + inner_top_down</span><br><span class="line">        results.insert(<span class="number">0</span>, self.get_result_from_layer_blocks(last_inner, idx))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.extra_blocks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        results, names = self.extra_blocks(results, x, names)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>具体步骤是从后往前计算每一层的result，即论文中的<span
class="math inline">\(P_n\)</span>：</p>
<ol type="1">
<li><code>inner_lateral</code>就是CNN的feature
map经过1×1卷积计算的结果，该卷积通过<code>self.get_result_from_inner_blocks(x[idx], idx)</code>实现；</li>
<li><code>inner_top_down</code>就是从后一层<span
class="math inline">\(P_{n+1}\)</span>上采样出来的结果，该上采样通过插值实现<code>F.interpolate(last_inner, size=feat_shape, mode="nearest")</code>；</li>
<li><code>last_inner</code>就是两者合并的结果，通过element-wise
addition实现；</li>
<li>在加入<code>results</code>前，还需要用3×3卷积计算一下，即<code>self.get_result_from_layer_blocks(last_inner, idx)</code>。</li>
</ol>
<p>最后，如果还有额外计算块的话，就再算一遍，取得这层的结果也加入。</p>
<p>在具体实现中，在FPN尾部增加了<code>LastLevelMaxPool</code>，并将其计算结果命名为<code>pool</code>加入了<code>names</code>。</p>
<p>本节的例子经过FPN部分的计算后，从ResNet输出的4个通道数不同的feature
maps，转换为了各层通道数一致的一个有序字典OrderedDict：</p>
<ol type="1">
<li><code>'0': shape[2, 256, 256, 272]</code>，源自ResNet的layer1；</li>
<li><code>'1': shape[2, 256, 128, 136]</code>，源自ResNet的layer2；</li>
<li><code>'2': shape[2, 256, 64, 68]</code>，源自ResNet的layer3；</li>
<li><code>'3': shape[2, 256, 32, 34]</code>，源自ResNet的layer4；</li>
<li><code>'pool': shape[2, 256, 16, 17]</code>，源自FPN作为<code>extra_blocks</code>的<code>LastLevelMaxPool</code>。</li>
</ol>
<h4 id="regionproposalnetwork">RegionProposalNetwork</h4>
<p><code>RegionProposalNetwork</code>在<code>torchvision.models.detection.rpn</code>包中实现。</p>
<p><code>RegionProposalNetwork</code>的实现比较长，主要看<code>__init__</code>和<code>__forward__</code>就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RegionProposalNetwork</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements Region Proposal Network (RPN).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature</span></span><br><span class="line"><span class="string">            maps.</span></span><br><span class="line"><span class="string">        head (nn.Module): module that computes the objectness and regression deltas</span></span><br><span class="line"><span class="string">        fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as positive during training of the RPN.</span></span><br><span class="line"><span class="string">        bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as negative during training of the RPN.</span></span><br><span class="line"><span class="string">        batch_size_per_image (int): number of anchors that are sampled during training of the RPN</span></span><br><span class="line"><span class="string">            for computing the loss</span></span><br><span class="line"><span class="string">        positive_fraction (float): proportion of positive anchors in a mini-batch during training</span></span><br><span class="line"><span class="string">            of the RPN</span></span><br><span class="line"><span class="string">        pre_nms_top_n (Dict[int]): number of proposals to keep before applying NMS. It should</span></span><br><span class="line"><span class="string">            contain two fields: training and testing, to allow for different values depending</span></span><br><span class="line"><span class="string">            on training or evaluation</span></span><br><span class="line"><span class="string">        post_nms_top_n (Dict[int]): number of proposals to keep after applying NMS. It should</span></span><br><span class="line"><span class="string">            contain two fields: training and testing, to allow for different values depending</span></span><br><span class="line"><span class="string">            on training or evaluation</span></span><br><span class="line"><span class="string">        nms_thresh (float): NMS threshold used for postprocessing the RPN proposals</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __annotations__ = &#123;</span><br><span class="line">        <span class="string">&#x27;box_coder&#x27;</span>: det_utils.BoxCoder,</span><br><span class="line">        <span class="string">&#x27;proposal_matcher&#x27;</span>: det_utils.Matcher,</span><br><span class="line">        <span class="string">&#x27;fg_bg_sampler&#x27;</span>: det_utils.BalancedPositiveNegativeSampler,</span><br><span class="line">        <span class="string">&#x27;pre_nms_top_n&#x27;</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">int</span>],</span><br><span class="line">        <span class="string">&#x27;post_nms_top_n&#x27;</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">int</span>],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 anchor_generator,</span></span><br><span class="line"><span class="params">                 head,</span></span><br><span class="line"><span class="params">                 <span class="comment">#</span></span></span><br><span class="line"><span class="params">                 fg_iou_thresh, bg_iou_thresh,</span></span><br><span class="line"><span class="params">                 batch_size_per_image, positive_fraction,</span></span><br><span class="line"><span class="params">                 <span class="comment">#</span></span></span><br><span class="line"><span class="params">                 pre_nms_top_n, post_nms_top_n, nms_thresh</span>):</span><br><span class="line">        <span class="built_in">super</span>(RegionProposalNetwork, self).__init__()</span><br><span class="line">        self.anchor_generator = anchor_generator</span><br><span class="line">        self.head = head</span><br><span class="line">        self.box_coder = det_utils.BoxCoder(weights=(<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># used during training</span></span><br><span class="line">        self.box_similarity = box_ops.box_iou</span><br><span class="line"></span><br><span class="line">        self.proposal_matcher = det_utils.Matcher(</span><br><span class="line">            fg_iou_thresh,</span><br><span class="line">            bg_iou_thresh,</span><br><span class="line">            allow_low_quality_matches=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(</span><br><span class="line">            batch_size_per_image, positive_fraction</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># used during testing</span></span><br><span class="line">        self._pre_nms_top_n = pre_nms_top_n</span><br><span class="line">        self._post_nms_top_n = post_nms_top_n</span><br><span class="line">        self.nms_thresh = nms_thresh</span><br><span class="line">        self.min_size = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pre_nms_top_n</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> self._pre_nms_top_n[<span class="string">&#x27;training&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> self._pre_nms_top_n[<span class="string">&#x27;testing&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">post_nms_top_n</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> self._post_nms_top_n[<span class="string">&#x27;training&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> self._post_nms_top_n[<span class="string">&#x27;testing&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">assign_targets_to_anchors</span>(<span class="params">self, anchors, targets</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        labels = []</span><br><span class="line">        matched_gt_boxes = []</span><br><span class="line">        <span class="keyword">for</span> anchors_per_image, targets_per_image <span class="keyword">in</span> <span class="built_in">zip</span>(anchors, targets):</span><br><span class="line">            gt_boxes = targets_per_image[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> gt_boxes.numel() == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Background image (negative example)</span></span><br><span class="line">                device = anchors_per_image.device</span><br><span class="line">                matched_gt_boxes_per_image = torch.zeros(anchors_per_image.shape, dtype=torch.float32, device=device)</span><br><span class="line">                labels_per_image = torch.zeros((anchors_per_image.shape[<span class="number">0</span>],), dtype=torch.float32, device=device)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                match_quality_matrix = self.box_similarity(gt_boxes, anchors_per_image)</span><br><span class="line">                matched_idxs = self.proposal_matcher(match_quality_matrix)</span><br><span class="line">                <span class="comment"># get the targets corresponding GT for each proposal</span></span><br><span class="line">                <span class="comment"># NB: need to clamp the indices because we can have a single</span></span><br><span class="line">                <span class="comment"># GT in the image, and matched_idxs can be -2, which goes</span></span><br><span class="line">                <span class="comment"># out of bounds</span></span><br><span class="line">                matched_gt_boxes_per_image = gt_boxes[matched_idxs.clamp(<span class="built_in">min</span>=<span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line">                labels_per_image = matched_idxs &gt;= <span class="number">0</span></span><br><span class="line">                labels_per_image = labels_per_image.to(dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Background (negative examples)</span></span><br><span class="line">                bg_indices = matched_idxs == self.proposal_matcher.BELOW_LOW_THRESHOLD</span><br><span class="line">                labels_per_image[bg_indices] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># discard indices that are between thresholds</span></span><br><span class="line">                inds_to_discard = matched_idxs == self.proposal_matcher.BETWEEN_THRESHOLDS</span><br><span class="line">                labels_per_image[inds_to_discard] = -<span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">            labels.append(labels_per_image)</span><br><span class="line">            matched_gt_boxes.append(matched_gt_boxes_per_image)</span><br><span class="line">        <span class="keyword">return</span> labels, matched_gt_boxes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_top_n_idx</span>(<span class="params">self, objectness, num_anchors_per_level</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, <span class="type">List</span>[<span class="built_in">int</span>]) -&gt; Tensor</span></span><br><span class="line">        r = []</span><br><span class="line">        offset = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ob <span class="keyword">in</span> objectness.split(num_anchors_per_level, <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">                num_anchors, pre_nms_top_n = _onnx_get_num_anchors_and_pre_nms_top_n(ob, self.pre_nms_top_n())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                num_anchors = ob.shape[<span class="number">1</span>]</span><br><span class="line">                pre_nms_top_n = <span class="built_in">min</span>(self.pre_nms_top_n(), num_anchors)</span><br><span class="line">            _, top_n_idx = ob.topk(pre_nms_top_n, dim=<span class="number">1</span>)</span><br><span class="line">            r.append(top_n_idx + offset)</span><br><span class="line">            offset += num_anchors</span><br><span class="line">        <span class="keyword">return</span> torch.cat(r, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter_proposals</span>(<span class="params">self, proposals, objectness, image_shapes, num_anchors_per_level</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, Tensor, <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]], <span class="type">List</span>[<span class="built_in">int</span>]) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        num_images = proposals.shape[<span class="number">0</span>]</span><br><span class="line">        device = proposals.device</span><br><span class="line">        <span class="comment"># do not backprop throught objectness</span></span><br><span class="line">        objectness = objectness.detach()</span><br><span class="line">        objectness = objectness.reshape(num_images, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        levels = [</span><br><span class="line">            torch.full((n,), idx, dtype=torch.int64, device=device)</span><br><span class="line">            <span class="keyword">for</span> idx, n <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_anchors_per_level)</span><br><span class="line">        ]</span><br><span class="line">        levels = torch.cat(levels, <span class="number">0</span>)</span><br><span class="line">        levels = levels.reshape(<span class="number">1</span>, -<span class="number">1</span>).expand_as(objectness)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># select top_n boxes independently per level before applying nms</span></span><br><span class="line">        top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)</span><br><span class="line"></span><br><span class="line">        image_range = torch.arange(num_images, device=device)</span><br><span class="line">        batch_idx = image_range[:, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        objectness = objectness[batch_idx, top_n_idx]</span><br><span class="line">        levels = levels[batch_idx, top_n_idx]</span><br><span class="line">        proposals = proposals[batch_idx, top_n_idx]</span><br><span class="line"></span><br><span class="line">        final_boxes = []</span><br><span class="line">        final_scores = []</span><br><span class="line">        <span class="keyword">for</span> boxes, scores, lvl, img_shape <span class="keyword">in</span> <span class="built_in">zip</span>(proposals, objectness, levels, image_shapes):</span><br><span class="line">            boxes = box_ops.clip_boxes_to_image(boxes, img_shape)</span><br><span class="line">            keep = box_ops.remove_small_boxes(boxes, self.min_size)</span><br><span class="line">            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]</span><br><span class="line">            <span class="comment"># non-maximum suppression, independently done per level</span></span><br><span class="line">            keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)</span><br><span class="line">            <span class="comment"># keep only topk scoring predictions</span></span><br><span class="line">            keep = keep[:self.post_nms_top_n()]</span><br><span class="line">            boxes, scores = boxes[keep], scores[keep]</span><br><span class="line">            final_boxes.append(boxes)</span><br><span class="line">            final_scores.append(scores)</span><br><span class="line">        <span class="keyword">return</span> final_boxes, final_scores</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">self, objectness, pred_bbox_deltas, labels, regression_targets</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, Tensor, <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            objectness (Tensor)</span></span><br><span class="line"><span class="string">            pred_bbox_deltas (Tensor)</span></span><br><span class="line"><span class="string">            labels (List[Tensor])</span></span><br><span class="line"><span class="string">            regression_targets (List[Tensor])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            objectness_loss (Tensor)</span></span><br><span class="line"><span class="string">            box_loss (Tensor)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)</span><br><span class="line">        sampled_pos_inds = torch.where(torch.cat(sampled_pos_inds, dim=<span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">        sampled_neg_inds = torch.where(torch.cat(sampled_neg_inds, dim=<span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        objectness = objectness.flatten()</span><br><span class="line"></span><br><span class="line">        labels = torch.cat(labels, dim=<span class="number">0</span>)</span><br><span class="line">        regression_targets = torch.cat(regression_targets, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        box_loss = det_utils.smooth_l1_loss(</span><br><span class="line">            pred_bbox_deltas[sampled_pos_inds],</span><br><span class="line">            regression_targets[sampled_pos_inds],</span><br><span class="line">            beta=<span class="number">1</span> / <span class="number">9</span>,</span><br><span class="line">            size_average=<span class="literal">False</span>,</span><br><span class="line">        ) / (sampled_inds.numel())</span><br><span class="line"></span><br><span class="line">        objectness_loss = F.binary_cross_entropy_with_logits(</span><br><span class="line">            objectness[sampled_inds], labels[sampled_inds]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> objectness_loss, box_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                images,       <span class="comment"># type: ImageList</span></span></span><br><span class="line"><span class="params">                features,     <span class="comment"># type: Dict[str, Tensor]</span></span></span><br><span class="line"><span class="params">                targets=<span class="literal">None</span>  <span class="comment"># type: Optional[List[Dict[str, Tensor]]]</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            images (ImageList): images for which we want to compute the predictions</span></span><br><span class="line"><span class="string">            features (OrderedDict[Tensor]): features computed from the images that are</span></span><br><span class="line"><span class="string">                used for computing the predictions. Each tensor in the list</span></span><br><span class="line"><span class="string">                correspond to different feature levels</span></span><br><span class="line"><span class="string">            targets (List[Dict[Tensor]]): ground-truth boxes present in the image (optional).</span></span><br><span class="line"><span class="string">                If provided, each element in the dict should contain a field `boxes`,</span></span><br><span class="line"><span class="string">                with the locations of the ground-truth boxes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            boxes (List[Tensor]): the predicted boxes from the RPN, one Tensor per</span></span><br><span class="line"><span class="string">                image.</span></span><br><span class="line"><span class="string">            losses (Dict[Tensor]): the losses for the model during training. During</span></span><br><span class="line"><span class="string">                testing, it is an empty dict.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># RPN uses all feature maps that are available</span></span><br><span class="line">        features = <span class="built_in">list</span>(features.values())</span><br><span class="line">        objectness, pred_bbox_deltas = self.head(features)</span><br><span class="line">        anchors = self.anchor_generator(images, features)</span><br><span class="line"></span><br><span class="line">        num_images = <span class="built_in">len</span>(anchors)</span><br><span class="line">        num_anchors_per_level_shape_tensors = [o[<span class="number">0</span>].shape <span class="keyword">for</span> o <span class="keyword">in</span> objectness]</span><br><span class="line">        num_anchors_per_level = [s[<span class="number">0</span>] * s[<span class="number">1</span>] * s[<span class="number">2</span>] <span class="keyword">for</span> s <span class="keyword">in</span> num_anchors_per_level_shape_tensors]</span><br><span class="line">        objectness, pred_bbox_deltas = \</span><br><span class="line">            concat_box_prediction_layers(objectness, pred_bbox_deltas)</span><br><span class="line">        <span class="comment"># apply pred_bbox_deltas to anchors to obtain the decoded proposals</span></span><br><span class="line">        <span class="comment"># note that we detach the deltas because Faster R-CNN do not backprop through</span></span><br><span class="line">        <span class="comment"># the proposals</span></span><br><span class="line">        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)</span><br><span class="line">        proposals = proposals.view(num_images, -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)</span><br><span class="line"></span><br><span class="line">        losses = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            labels, matched_gt_boxes = self.assign_targets_to_anchors(anchors, targets)</span><br><span class="line">            regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)</span><br><span class="line">            loss_objectness, loss_rpn_box_reg = self.compute_loss(</span><br><span class="line">                objectness, pred_bbox_deltas, labels, regression_targets)</span><br><span class="line">            losses = &#123;</span><br><span class="line">                <span class="string">&quot;loss_objectness&quot;</span>: loss_objectness,</span><br><span class="line">                <span class="string">&quot;loss_rpn_box_reg&quot;</span>: loss_rpn_box_reg,</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> boxes, losses</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>主要看<code>__forward__</code>中的计算流，RPN的完整过程</p>
<ol type="1">
<li>把输入的特征features输入到RPNHead（<code>self.head</code>）中，输出object/non-object分类分值（<code>objectness</code>）和bbox回归数值（<code>pred_bbox_deltas</code>）；</li>
<li><code>self.anchor_generator</code>为当前输入的图像和feature
map生成<code>anchors</code>；</li>
<li><code>self.box_coder.decode</code>把bbox回归数值<code>pred_bbox_deltas</code>算到锚框<code>anchors</code>上，得到预测出的候选框<code>proposals</code>；</li>
<li>计算出的<code>proposals</code>可能很多且相互密集重叠，那么就通过<code>self.filter_proposals</code>做一遍过滤，输出候选框<code>proposals</code>和与之对应的分值<code>scores</code>；</li>
<li>如果是训练时，当然在RPN阶段需要根据预测出的<code>proposals</code>与候选框真值之间的误差来计算损失。</li>
</ol>
<h5 id="anchorgenerator">AnchorGenerator</h5>
<p><code>AnchorGenerator</code>在<code>torchvision.models.detection.anchor_utils</code>中实现，其作用是根据预定义的anchor的sizes和aspect_ratios，针对图像到feature
map的尺寸比例，计算feature map对应的anchors。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AnchorGenerator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Module that generates anchors for a set of feature maps and</span></span><br><span class="line"><span class="string">    image sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The module support computing anchors at multiple sizes and aspect ratios</span></span><br><span class="line"><span class="string">    per feature map. This module assumes aspect ratio = height / width for</span></span><br><span class="line"><span class="string">    each anchor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sizes and aspect_ratios should have the same number of elements, and it should</span></span><br><span class="line"><span class="string">    correspond to the number of feature maps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sizes[i] and aspect_ratios[i] can have an arbitrary number of elements,</span></span><br><span class="line"><span class="string">    and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors</span></span><br><span class="line"><span class="string">    per spatial location for feature map i.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        sizes (Tuple[Tuple[int]]):</span></span><br><span class="line"><span class="string">        aspect_ratios (Tuple[Tuple[float]]):</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    __annotations__ = &#123;</span><br><span class="line">        <span class="string">&quot;cell_anchors&quot;</span>: <span class="type">Optional</span>[<span class="type">List</span>[torch.Tensor]],</span><br><span class="line">        <span class="string">&quot;_cache&quot;</span>: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">List</span>[torch.Tensor]]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        sizes=(<span class="params">(<span class="params"><span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span></span>),</span>),</span></span><br><span class="line"><span class="params">        aspect_ratios=(<span class="params">(<span class="params"><span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span></span>),</span>),</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(AnchorGenerator, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(sizes[<span class="number">0</span>], (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="comment"># TODO change this</span></span><br><span class="line">            sizes = <span class="built_in">tuple</span>((s,) <span class="keyword">for</span> s <span class="keyword">in</span> sizes)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(aspect_ratios[<span class="number">0</span>], (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            aspect_ratios = (aspect_ratios,) * <span class="built_in">len</span>(sizes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(sizes) == <span class="built_in">len</span>(aspect_ratios)</span><br><span class="line"></span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.aspect_ratios = aspect_ratios</span><br><span class="line">        self.cell_anchors = <span class="literal">None</span></span><br><span class="line">        self._cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> https://github.com/pytorch/pytorch/issues/26792</span></span><br><span class="line">    <span class="comment"># For every (aspect_ratios, scales) combination, output a zero-centered anchor with those values.</span></span><br><span class="line">    <span class="comment"># (scales, aspect_ratios) are usually an element of zip(self.scales, self.aspect_ratios)</span></span><br><span class="line">    <span class="comment"># This method assumes aspect ratio = height / width for an anchor.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_anchors</span>(<span class="params">self, scales, aspect_ratios, dtype=torch.float32, device=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">List</span>[<span class="built_in">float</span>], <span class="built_in">int</span>, Device) -&gt; Tensor  # noqa: F821</span></span><br><span class="line">        scales = torch.as_tensor(scales, dtype=dtype, device=device)</span><br><span class="line">        aspect_ratios = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)</span><br><span class="line">        h_ratios = torch.sqrt(aspect_ratios)</span><br><span class="line">        w_ratios = <span class="number">1</span> / h_ratios</span><br><span class="line"></span><br><span class="line">        ws = (w_ratios[:, <span class="literal">None</span>] * scales[<span class="literal">None</span>, :]).view(-<span class="number">1</span>)</span><br><span class="line">        hs = (h_ratios[:, <span class="literal">None</span>] * scales[<span class="literal">None</span>, :]).view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> base_anchors.<span class="built_in">round</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_cell_anchors</span>(<span class="params">self, dtype, device</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="built_in">int</span>, Device) -&gt; <span class="literal">None</span>  # noqa: F821</span></span><br><span class="line">        <span class="keyword">if</span> self.cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cell_anchors = self.cell_anchors</span><br><span class="line">            <span class="keyword">assert</span> cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="comment"># suppose that all anchors have the same device</span></span><br><span class="line">            <span class="comment"># which is a valid assumption in the current state of the codebase</span></span><br><span class="line">            <span class="keyword">if</span> cell_anchors[<span class="number">0</span>].device == device:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        cell_anchors = [</span><br><span class="line">            self.generate_anchors(</span><br><span class="line">                sizes,</span><br><span class="line">                aspect_ratios,</span><br><span class="line">                dtype,</span><br><span class="line">                device</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> sizes, aspect_ratios <span class="keyword">in</span> <span class="built_in">zip</span>(self.sizes, self.aspect_ratios)</span><br><span class="line">        ]</span><br><span class="line">        self.cell_anchors = cell_anchors</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_anchors_per_location</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">len</span>(s) * <span class="built_in">len</span>(a) <span class="keyword">for</span> s, a <span class="keyword">in</span> <span class="built_in">zip</span>(self.sizes, self.aspect_ratios)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For every combination of (a, (g, s), i) in (self.cell_anchors, zip(grid_sizes, strides), 0:2),</span></span><br><span class="line">    <span class="comment"># output g[i] anchors that are s[i] distance apart in direction i, with the same dimensions as a.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">grid_anchors</span>(<span class="params">self, grid_sizes, strides</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], <span class="type">List</span>[<span class="type">List</span>[Tensor]]) -&gt; <span class="type">List</span>[Tensor]</span></span><br><span class="line">        anchors = []</span><br><span class="line">        cell_anchors = self.cell_anchors</span><br><span class="line">        <span class="keyword">assert</span> cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(grid_sizes) == <span class="built_in">len</span>(strides) == <span class="built_in">len</span>(cell_anchors)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> size, stride, base_anchors <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            grid_sizes, strides, cell_anchors</span><br><span class="line">        ):</span><br><span class="line">            grid_height, grid_width = size</span><br><span class="line">            stride_height, stride_width = stride</span><br><span class="line">            device = base_anchors.device</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For output anchor, compute [x_center, y_center, x_center, y_center]</span></span><br><span class="line">            shifts_x = torch.arange(</span><br><span class="line">                <span class="number">0</span>, grid_width, dtype=torch.float32, device=device</span><br><span class="line">            ) * stride_width</span><br><span class="line">            shifts_y = torch.arange(</span><br><span class="line">                <span class="number">0</span>, grid_height, dtype=torch.float32, device=device</span><br><span class="line">            ) * stride_height</span><br><span class="line">            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)</span><br><span class="line">            shift_x = shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line">            shift_y = shift_y.reshape(-<span class="number">1</span>)</span><br><span class="line">            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For every (base anchor, output anchor) pair,</span></span><br><span class="line">            <span class="comment"># offset each zero-centered base anchor by the center of the output anchor.</span></span><br><span class="line">            anchors.append(</span><br><span class="line">                (shifts.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>) + base_anchors.view(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">4</span>)).reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> anchors</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cached_grid_anchors</span>(<span class="params">self, grid_sizes, strides</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], <span class="type">List</span>[<span class="type">List</span>[Tensor]]) -&gt; <span class="type">List</span>[Tensor]</span></span><br><span class="line">        key = <span class="built_in">str</span>(grid_sizes) + <span class="built_in">str</span>(strides)</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self._cache:</span><br><span class="line">            <span class="keyword">return</span> self._cache[key]</span><br><span class="line">        anchors = self.grid_anchors(grid_sizes, strides)</span><br><span class="line">        self._cache[key] = anchors</span><br><span class="line">        <span class="keyword">return</span> anchors</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image_list, feature_maps</span>):</span><br><span class="line">        <span class="comment"># type: (ImageList, <span class="type">List</span>[Tensor]) -&gt; <span class="type">List</span>[Tensor]</span></span><br><span class="line">        grid_sizes = <span class="built_in">list</span>([feature_map.shape[-<span class="number">2</span>:] <span class="keyword">for</span> feature_map <span class="keyword">in</span> feature_maps])</span><br><span class="line">        image_size = image_list.tensors.shape[-<span class="number">2</span>:]</span><br><span class="line">        dtype, device = feature_maps[<span class="number">0</span>].dtype, feature_maps[<span class="number">0</span>].device</span><br><span class="line">        strides = [[torch.tensor(image_size[<span class="number">0</span>] // g[<span class="number">0</span>], dtype=torch.int64, device=device),</span><br><span class="line">                    torch.tensor(image_size[<span class="number">1</span>] // g[<span class="number">1</span>], dtype=torch.int64, device=device)] <span class="keyword">for</span> g <span class="keyword">in</span> grid_sizes]</span><br><span class="line">        self.set_cell_anchors(dtype, device)</span><br><span class="line">        anchors_over_all_feature_maps = self.cached_grid_anchors(grid_sizes, strides)</span><br><span class="line">        anchors = torch.jit.annotate(<span class="type">List</span>[<span class="type">List</span>[torch.Tensor]], [])</span><br><span class="line">        <span class="keyword">for</span> i, (image_height, image_width) <span class="keyword">in</span> <span class="built_in">enumerate</span>(image_list.image_sizes):</span><br><span class="line">            anchors_in_image = []</span><br><span class="line">            <span class="keyword">for</span> anchors_per_feature_map <span class="keyword">in</span> anchors_over_all_feature_maps:</span><br><span class="line">                anchors_in_image.append(anchors_per_feature_map)</span><br><span class="line">            anchors.append(anchors_in_image)</span><br><span class="line">        anchors = [torch.cat(anchors_per_image) <span class="keyword">for</span> anchors_per_image <span class="keyword">in</span> anchors]</span><br><span class="line">        <span class="comment"># Clear the cache in case that memory leaks.</span></span><br><span class="line">        self._cache.clear()</span><br><span class="line">        <span class="keyword">return</span> anchors</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这部分代码也有点长，主要原理也还是看<code>__forward__</code>计算流即可，这里面有一系列预备的计算，随后就是两层的for循环，表示：每一个图片可以传入<span
class="math inline">\(n\)</span>个（尺寸不同的）feature
map，每一个feature map上都有<span
class="math inline">\(k_i\)</span>个anchor，那么每个图片就有<span
class="math inline">\(K = \sum_{i=1}^{n}k_i\)</span>个anchors。
其中，每个滑窗位置上有<span
class="math inline">\(A\)</span>个anchor，第<span
class="math inline">\(i\)</span>个feature map上有<span
class="math inline">\(L_i\)</span>个滑窗位置，则该层feature map上有<span
class="math inline">\(k_i = A L_i\)</span>个anchors。</p>
<p>两层循环，外层遍历images，内层遍历feature
maps，由此输出所有图片的feature map上的anchors。</p>
<p>在具体实现中，AnchorGenerator：</p>
<ol type="1">
<li><code>self.set_cell_anchors</code>函数负责为每一层feature
map生成<code>self.cell_anchors</code>，这个Cell
Anchors的尺寸基于的是输入图片tensor的尺寸，；</li>
<li><code>self.cached_grid_anchors</code>函数内会进一步调用<code>self.grid_anchors</code>函数，该函数负责根据feature
map的网格尺寸以及该feature
map相较于输入图片tensor的步长，计算出<code>anchors_over_all_feature_maps</code>，它的尺寸则是基于输入图片tensor的尺寸。</li>
<li>双重for循环，输入<span
class="math inline">\(N\)</span>个图片，就相应地将anchors复制出<span
class="math inline">\(N\)</span>份。</li>
<li>最后<code>torch.cat</code>拉平每个图片上不同feature map上的所有<span
class="math inline">\(K\)</span>个anchors，形成一个长度为<span
class="math inline">\(N\)</span>的list，每个元素是<span
class="math inline">\(K \times 4\)</span>的anchors张量。</li>
</ol>
<p>对应到例子，<code>torchvision</code>实现默认为：</p>
<ol type="1">
<li>3种aspect ratio，分别为0.5, 1.0, 2.0；</li>
<li>每层feature map对应1个scale，5层feature map分别为16, 32, 64, 128,
256。</li>
</ol>
<p>因此，<code>cell_anchors</code>中，每层feature map都是3个anchor
cells：</p>
<ol type="1">
<li><code>'0'</code>: shape[3, 4]</li>
<li><code>'1'</code>: shape[3, 4]</li>
<li><code>'2'</code>: shape[3, 4]</li>
<li><code>'3'</code>: shape[3, 4]</li>
<li><code>'4'</code>: shape[3, 4]</li>
</ol>
<p>结合例子来算，把<code>cell_anchors</code>算到输入图像张量的每一个滑窗位置上，就可以算出所有位置上的所有<code>anchors_over_all_feature_maps</code>：</p>
<ol type="1">
<li><code>'0'</code>: shape[208896, 4]，<span
class="math inline">\(208896 = 256 \times 272 \times 3\)</span>；</li>
<li><code>'1'</code>: shape[52224, 4]，<span class="math inline">\(52224
= 128 \times 136 \times 3\)</span>；</li>
<li><code>'2'</code>: shape[13056, 4]，<span class="math inline">\(13056
= 64 \times 68 \times 3\)</span>；</li>
<li><code>'3'</code>: shape[3264, 4]，<span class="math inline">\(3264 =
32 \times 34 \times 3\)</span>；</li>
<li><code>'4'</code>: shape[816, 4]，<span class="math inline">\(816 =
16 \times 17 \times 3\)</span>；</li>
</ol>
<p>最后返回的<code>anchors</code>会为输入的每个图片复制一份，并通过<code>torch.cat</code>拉平：</p>
<ol type="1">
<li>shape[278256, 4], <span class="math inline">\(278256 = 208896 +
52224 + 13056 + 3264 + 816\)</span>；</li>
<li>shape[278256, 4], <span class="math inline">\(278256 = 208896 +
52224 + 13056 + 3264 + 816\)</span>；</li>
</ol>
<h5 id="rpnhead">RPNHead</h5>
<p><code>RPNHead</code>在<code>torchvision.models.detection.rpn</code>包中实现。RPNHead被用于以滑窗的形式在特征提取出的feature
map上滑动并计算每个anchor的bbox回归值和object/non-object二分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RPNHead</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Adds a simple RPN Head with classification and regression heads</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels (int): number of channels of the input feature</span></span><br><span class="line"><span class="string">        num_anchors (int): number of anchors to be predicted</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_anchors</span>):</span><br><span class="line">        <span class="built_in">super</span>(RPNHead, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(</span><br><span class="line">            in_channels, in_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.bbox_pred = nn.Conv2d(</span><br><span class="line">            in_channels, num_anchors * <span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.children():</span><br><span class="line">            torch.nn.init.normal_(layer.weight, std=<span class="number">0.01</span>)</span><br><span class="line">            torch.nn.init.constant_(layer.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor]) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        logits = []</span><br><span class="line">        bbox_reg = []</span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> x:</span><br><span class="line">            t = F.relu(self.conv(feature))</span><br><span class="line">            logits.append(self.cls_logits(t))</span><br><span class="line">            bbox_reg.append(self.bbox_pred(t))</span><br><span class="line">        <span class="keyword">return</span> logits, bbox_reg</span><br></pre></td></tr></table></figure>
<p>可以看到，RPNHead的结构不复杂，就是三个卷积：</p>
<ol type="1">
<li><code>self.conv</code>：3×3卷积，对输入的feature
map做卷积处理；</li>
<li><code>self.cls_logits</code>：1×1卷积，对处理后的feature map <span
class="math inline">\(t\)</span>做卷积，取得object/non-object的分类数值；</li>
<li><code>self.bbox_reg</code>：1×1卷积，对处理后的feature map <span
class="math inline">\(t\)</span>做卷积，取得bbox坐标值的回归数值。</li>
</ol>
<p>在<code>forward</code>前向传播计算的时候，输入的x是一个<code>List[Tensor]</code>，即FPN的输出。值得注意的是，<code>for</code>循环遍历的并不是每一张图片，而是FPN输出的每一层特征。</p>
<p>在本例中，RPNHead的两个卷积分支输出了两个<code>List[Tensor]</code>：</p>
<p><code>logits</code> (<code>objectness</code>):</p>
<ol type="1">
<li><code>'0'</code>: shape[2, 3, 252, 272]；</li>
<li><code>'1'</code>: shape[2, 3, 128, 136]；</li>
<li><code>'2'</code>: shape[2, 3, 64, 68]；</li>
<li><code>'3'</code>: shape[2, 3, 32, 34]；</li>
<li><code>'4'</code>: shape[2, 3, 16, 17]；</li>
</ol>
<p><code>bbox_regs</code> (<code>pred_bbox_deltas</code>):</p>
<ol type="1">
<li><code>'0'</code>: shape[2, 12, 252, 272]；</li>
<li><code>'1'</code>: shape[2, 12, 128, 136]；</li>
<li><code>'2'</code>: shape[2, 12, 64, 68]；</li>
<li><code>'3'</code>: shape[2, 12, 32, 34]；</li>
<li><code>'4'</code>: shape[2, 12, 16, 17]；</li>
</ol>
<p>因为每个滑窗位置对应三种<code>ratios</code>，即3个anchors，所以<code>logits</code>是3个值，而<code>bbox_regs</code>因为坐标乘4，所以是12个值。</p>
<h5 id="boxcoder">BoxCoder</h5>
<p><code>BoxCoder</code>在<code>torchvision.models.detection._utils</code>中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BoxCoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This class encodes and decodes a set of bounding boxes into</span></span><br><span class="line"><span class="string">    the representation used for training the regressors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weights, bbox_xform_clip=math.log(<span class="params"><span class="number">1000.</span> / <span class="number">16</span></span>)</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">Tuple</span>[<span class="built_in">float</span>, <span class="built_in">float</span>, <span class="built_in">float</span>, <span class="built_in">float</span>], <span class="built_in">float</span>) -&gt; <span class="literal">None</span></span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            weights (4-element tuple)</span></span><br><span class="line"><span class="string">            bbox_xform_clip (float)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.weights = weights</span><br><span class="line">        self.bbox_xform_clip = bbox_xform_clip</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, reference_boxes, proposals</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]) -&gt; <span class="type">List</span>[Tensor]</span></span><br><span class="line">        boxes_per_image = [<span class="built_in">len</span>(b) <span class="keyword">for</span> b <span class="keyword">in</span> reference_boxes]</span><br><span class="line">        reference_boxes = torch.cat(reference_boxes, dim=<span class="number">0</span>)</span><br><span class="line">        proposals = torch.cat(proposals, dim=<span class="number">0</span>)</span><br><span class="line">        targets = self.encode_single(reference_boxes, proposals)</span><br><span class="line">        <span class="keyword">return</span> targets.split(boxes_per_image, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_single</span>(<span class="params">self, reference_boxes, proposals</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Encode a set of proposals with respect to some</span></span><br><span class="line"><span class="string">        reference boxes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            reference_boxes (Tensor): reference boxes</span></span><br><span class="line"><span class="string">            proposals (Tensor): boxes to be encoded</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dtype = reference_boxes.dtype</span><br><span class="line">        device = reference_boxes.device</span><br><span class="line">        weights = torch.as_tensor(self.weights, dtype=dtype, device=device)</span><br><span class="line">        targets = encode_boxes(reference_boxes, proposals, weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> targets</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, rel_codes, boxes</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, <span class="type">List</span>[Tensor]) -&gt; Tensor</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(boxes, (<span class="built_in">list</span>, <span class="built_in">tuple</span>))</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(rel_codes, torch.Tensor)</span><br><span class="line">        boxes_per_image = [b.size(<span class="number">0</span>) <span class="keyword">for</span> b <span class="keyword">in</span> boxes]</span><br><span class="line">        concat_boxes = torch.cat(boxes, dim=<span class="number">0</span>)</span><br><span class="line">        box_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> val <span class="keyword">in</span> boxes_per_image:</span><br><span class="line">            box_sum += val</span><br><span class="line">        pred_boxes = self.decode_single(</span><br><span class="line">            rel_codes.reshape(box_sum, -<span class="number">1</span>), concat_boxes</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> pred_boxes.reshape(box_sum, -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_single</span>(<span class="params">self, rel_codes, boxes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        From a set of original boxes and encoded relative box offsets,</span></span><br><span class="line"><span class="string">        get the decoded boxes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            rel_codes (Tensor): encoded boxes</span></span><br><span class="line"><span class="string">            boxes (Tensor): reference boxes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        boxes = boxes.to(rel_codes.dtype)</span><br><span class="line"></span><br><span class="line">        widths = boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>]</span><br><span class="line">        heights = boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]</span><br><span class="line">        ctr_x = boxes[:, <span class="number">0</span>] + <span class="number">0.5</span> * widths</span><br><span class="line">        ctr_y = boxes[:, <span class="number">1</span>] + <span class="number">0.5</span> * heights</span><br><span class="line"></span><br><span class="line">        wx, wy, ww, wh = self.weights</span><br><span class="line">        dx = rel_codes[:, <span class="number">0</span>::<span class="number">4</span>] / wx</span><br><span class="line">        dy = rel_codes[:, <span class="number">1</span>::<span class="number">4</span>] / wy</span><br><span class="line">        dw = rel_codes[:, <span class="number">2</span>::<span class="number">4</span>] / ww</span><br><span class="line">        dh = rel_codes[:, <span class="number">3</span>::<span class="number">4</span>] / wh</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prevent sending too large values into torch.exp()</span></span><br><span class="line">        dw = torch.clamp(dw, <span class="built_in">max</span>=self.bbox_xform_clip)</span><br><span class="line">        dh = torch.clamp(dh, <span class="built_in">max</span>=self.bbox_xform_clip)</span><br><span class="line"></span><br><span class="line">        pred_ctr_x = dx * widths[:, <span class="literal">None</span>] + ctr_x[:, <span class="literal">None</span>]</span><br><span class="line">        pred_ctr_y = dy * heights[:, <span class="literal">None</span>] + ctr_y[:, <span class="literal">None</span>]</span><br><span class="line">        pred_w = torch.exp(dw) * widths[:, <span class="literal">None</span>]</span><br><span class="line">        pred_h = torch.exp(dh) * heights[:, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        pred_boxes1 = pred_ctr_x - torch.tensor(<span class="number">0.5</span>, dtype=pred_ctr_x.dtype, device=pred_w.device) * pred_w</span><br><span class="line">        pred_boxes2 = pred_ctr_y - torch.tensor(<span class="number">0.5</span>, dtype=pred_ctr_y.dtype, device=pred_h.device) * pred_h</span><br><span class="line">        pred_boxes3 = pred_ctr_x + torch.tensor(<span class="number">0.5</span>, dtype=pred_ctr_x.dtype, device=pred_w.device) * pred_w</span><br><span class="line">        pred_boxes4 = pred_ctr_y + torch.tensor(<span class="number">0.5</span>, dtype=pred_ctr_y.dtype, device=pred_h.device) * pred_h</span><br><span class="line">        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=<span class="number">2</span>).flatten(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_boxes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>RPN中<code>self.box_coder</code>使用BoxCoder作为bbox的编解码器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)</span><br></pre></td></tr></table></figure>
<p>通过BoxCoder实例，将RPNHead回归出的<code>pred_bbox_deltas</code>与RPN的锚框<code>anchors</code>做解码计算，把回归出的偏移值加到基准anchors位置上，解码输出候选框<code>proposals</code>。</p>
<p>在本例中，RPN的<code>forward</code>对解码出的原始<code>proposals</code>做了维度整理<code>proposals = proposals.view(num_images, -1, 4)</code>，得到的<code>proposals</code>是：</p>
<ul>
<li>shape[2, 278256, 4]</li>
</ul>
<h5 id="filter_proposals">filter_proposals</h5>
<p><code>filter_proposals</code>是一个对RPN
Head生成的候选框<code>proposals</code>的过滤操作，在RPN类<code>RegionProposalNetwork</code>中作为成员函数实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RegionProposalNetwork</span>(torch.nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">filter_proposals</span>(<span class="params">self, proposals, objectness, image_shapes, num_anchors_per_level</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, Tensor, <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]], <span class="type">List</span>[<span class="built_in">int</span>]) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        num_images = proposals.shape[<span class="number">0</span>]</span><br><span class="line">        device = proposals.device</span><br><span class="line">        <span class="comment"># do not backprop throught objectness</span></span><br><span class="line">        objectness = objectness.detach()</span><br><span class="line">        objectness = objectness.reshape(num_images, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        levels = [</span><br><span class="line">            torch.full((n,), idx, dtype=torch.int64, device=device)</span><br><span class="line">            <span class="keyword">for</span> idx, n <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_anchors_per_level)</span><br><span class="line">        ]</span><br><span class="line">        levels = torch.cat(levels, <span class="number">0</span>)</span><br><span class="line">        levels = levels.reshape(<span class="number">1</span>, -<span class="number">1</span>).expand_as(objectness)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># select top_n boxes independently per level before applying nms</span></span><br><span class="line">        top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)</span><br><span class="line"></span><br><span class="line">        image_range = torch.arange(num_images, device=device)</span><br><span class="line">        batch_idx = image_range[:, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">        objectness = objectness[batch_idx, top_n_idx]</span><br><span class="line">        levels = levels[batch_idx, top_n_idx]</span><br><span class="line">        proposals = proposals[batch_idx, top_n_idx]</span><br><span class="line"></span><br><span class="line">        final_boxes = []</span><br><span class="line">        final_scores = []</span><br><span class="line">        <span class="keyword">for</span> boxes, scores, lvl, img_shape <span class="keyword">in</span> <span class="built_in">zip</span>(proposals, objectness, levels, image_shapes):</span><br><span class="line">            boxes = box_ops.clip_boxes_to_image(boxes, img_shape)</span><br><span class="line">            keep = box_ops.remove_small_boxes(boxes, self.min_size)</span><br><span class="line">            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]</span><br><span class="line">            <span class="comment"># non-maximum suppression, independently done per level</span></span><br><span class="line">            keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)</span><br><span class="line">            <span class="comment"># keep only topk scoring predictions</span></span><br><span class="line">            keep = keep[:self.post_nms_top_n()]</span><br><span class="line">            boxes, scores = boxes[keep], scores[keep]</span><br><span class="line">            final_boxes.append(boxes)</span><br><span class="line">            final_scores.append(scores)</span><br><span class="line">        <span class="keyword">return</span> final_boxes, final_scores</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对<code>proposals</code>的过滤操作分几个阶段实现：</p>
<ol type="1">
<li>首先，根据<code>objectness</code>分值和<code>num_anchors_per_level</code>来在每层选出<code>top_n_idx(pre_nms_top_n)</code>用于在NMS前先筛选一下<code>proposals</code>；</li>
<li>此后，进入for循环，遍历batch中的每张图片，
<ol type="1">
<li>先做一些裁边界、去小框的处理；</li>
<li>然后再做（类间）NMS；</li>
<li>保留当前图片所有结果的<code>post_nms_top_n</code>的目标作为返回结果。</li>
</ol></li>
</ol>
<p>最后将这么多筛选操作筛选出的<code>final_boxes</code>和<code>final_scores</code>返回（<code>boxes</code>是筛选后的<code>proposals</code>，<code>scores</code>是筛选后的<code>objectness</code>）。</p>
<p>在本例中，有两张图片，每张图片上有278256个<code>anchors</code>，因此产生278256个<code>proposals</code>和<code>objectness</code>，进过筛选处理后：</p>
<ol type="1">
<li><code>final_boxes</code>：
<ol type="1">
<li>shape[1000, 4]</li>
<li>shape[1000, 4]</li>
</ol></li>
<li><code>final_scores</code>:
<ol type="1">
<li>shape 1000</li>
<li>shape 1000</li>
</ol></li>
</ol>
<p>因为FasterRCNN中默认值<code>rpn_post_nms_top_n_test=1000</code>，所以在eval模式（即test,
infer情况）下，例子中的两张图片都各筛选出了top-1000个boxes。</p>
<h4 id="roiheads">RoIHeads</h4>
<p><code>RoIHeads</code>在<code>torchvision.models.detection.roi_heads</code>包中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RoIHeads</span>(torch.nn.Module):</span><br><span class="line">    __annotations__ = &#123;</span><br><span class="line">        <span class="string">&#x27;box_coder&#x27;</span>: det_utils.BoxCoder,</span><br><span class="line">        <span class="string">&#x27;proposal_matcher&#x27;</span>: det_utils.Matcher,</span><br><span class="line">        <span class="string">&#x27;fg_bg_sampler&#x27;</span>: det_utils.BalancedPositiveNegativeSampler,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 box_roi_pool,</span></span><br><span class="line"><span class="params">                 box_head,</span></span><br><span class="line"><span class="params">                 box_predictor,</span></span><br><span class="line"><span class="params">                 <span class="comment"># Faster R-CNN training</span></span></span><br><span class="line"><span class="params">                 fg_iou_thresh, bg_iou_thresh,</span></span><br><span class="line"><span class="params">                 batch_size_per_image, positive_fraction,</span></span><br><span class="line"><span class="params">                 bbox_reg_weights,</span></span><br><span class="line"><span class="params">                 <span class="comment"># Faster R-CNN inference</span></span></span><br><span class="line"><span class="params">                 score_thresh,</span></span><br><span class="line"><span class="params">                 nms_thresh,</span></span><br><span class="line"><span class="params">                 detections_per_img,</span></span><br><span class="line"><span class="params">                 <span class="comment"># Mask</span></span></span><br><span class="line"><span class="params">                 mask_roi_pool=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 mask_head=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 mask_predictor=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 keypoint_roi_pool=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 keypoint_head=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 keypoint_predictor=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>(RoIHeads, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.box_similarity = box_ops.box_iou</span><br><span class="line">        <span class="comment"># assign ground-truth boxes for each proposal</span></span><br><span class="line">        self.proposal_matcher = det_utils.Matcher(</span><br><span class="line">            fg_iou_thresh,</span><br><span class="line">            bg_iou_thresh,</span><br><span class="line">            allow_low_quality_matches=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(</span><br><span class="line">            batch_size_per_image,</span><br><span class="line">            positive_fraction)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bbox_reg_weights <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            bbox_reg_weights = (<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">5.</span>, <span class="number">5.</span>)</span><br><span class="line">        self.box_coder = det_utils.BoxCoder(bbox_reg_weights)</span><br><span class="line"></span><br><span class="line">        self.box_roi_pool = box_roi_pool</span><br><span class="line">        self.box_head = box_head</span><br><span class="line">        self.box_predictor = box_predictor</span><br><span class="line"></span><br><span class="line">        self.score_thresh = score_thresh</span><br><span class="line">        self.nms_thresh = nms_thresh</span><br><span class="line">        self.detections_per_img = detections_per_img</span><br><span class="line"></span><br><span class="line">        self.mask_roi_pool = mask_roi_pool</span><br><span class="line">        self.mask_head = mask_head</span><br><span class="line">        self.mask_predictor = mask_predictor</span><br><span class="line"></span><br><span class="line">        self.keypoint_roi_pool = keypoint_roi_pool</span><br><span class="line">        self.keypoint_head = keypoint_head</span><br><span class="line">        self.keypoint_predictor = keypoint_predictor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_mask</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.mask_roi_pool <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> self.mask_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> self.mask_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_keypoint</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.keypoint_roi_pool <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> self.keypoint_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> self.keypoint_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">assign_targets_to_proposals</span>(<span class="params">self, proposals, gt_boxes, gt_labels</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        matched_idxs = []</span><br><span class="line">        labels = []</span><br><span class="line">        <span class="keyword">for</span> proposals_in_image, gt_boxes_in_image, gt_labels_in_image <span class="keyword">in</span> <span class="built_in">zip</span>(proposals, gt_boxes, gt_labels):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> gt_boxes_in_image.numel() == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Background image</span></span><br><span class="line">                device = proposals_in_image.device</span><br><span class="line">                clamped_matched_idxs_in_image = torch.zeros(</span><br><span class="line">                    (proposals_in_image.shape[<span class="number">0</span>],), dtype=torch.int64, device=device</span><br><span class="line">                )</span><br><span class="line">                labels_in_image = torch.zeros(</span><br><span class="line">                    (proposals_in_image.shape[<span class="number">0</span>],), dtype=torch.int64, device=device</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment">#  set to self.box_similarity when https://github.com/pytorch/pytorch/issues/27495 lands</span></span><br><span class="line">                match_quality_matrix = box_ops.box_iou(gt_boxes_in_image, proposals_in_image)</span><br><span class="line">                matched_idxs_in_image = self.proposal_matcher(match_quality_matrix)</span><br><span class="line"></span><br><span class="line">                clamped_matched_idxs_in_image = matched_idxs_in_image.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                labels_in_image = gt_labels_in_image[clamped_matched_idxs_in_image]</span><br><span class="line">                labels_in_image = labels_in_image.to(dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Label background (below the low threshold)</span></span><br><span class="line">                bg_inds = matched_idxs_in_image == self.proposal_matcher.BELOW_LOW_THRESHOLD</span><br><span class="line">                labels_in_image[bg_inds] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Label ignore proposals (between low and high thresholds)</span></span><br><span class="line">                ignore_inds = matched_idxs_in_image == self.proposal_matcher.BETWEEN_THRESHOLDS</span><br><span class="line">                labels_in_image[ignore_inds] = -<span class="number">1</span>  <span class="comment"># -1 is ignored by sampler</span></span><br><span class="line"></span><br><span class="line">            matched_idxs.append(clamped_matched_idxs_in_image)</span><br><span class="line">            labels.append(labels_in_image)</span><br><span class="line">        <span class="keyword">return</span> matched_idxs, labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">subsample</span>(<span class="params">self, labels</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor]) -&gt; <span class="type">List</span>[Tensor]</span></span><br><span class="line">        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)</span><br><span class="line">        sampled_inds = []</span><br><span class="line">        <span class="keyword">for</span> img_idx, (pos_inds_img, neg_inds_img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">            <span class="built_in">zip</span>(sampled_pos_inds, sampled_neg_inds)</span><br><span class="line">        ):</span><br><span class="line">            img_sampled_inds = torch.where(pos_inds_img | neg_inds_img)[<span class="number">0</span>]</span><br><span class="line">            sampled_inds.append(img_sampled_inds)</span><br><span class="line">        <span class="keyword">return</span> sampled_inds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_gt_proposals</span>(<span class="params">self, proposals, gt_boxes</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]) -&gt; <span class="type">List</span>[Tensor]</span></span><br><span class="line">        proposals = [</span><br><span class="line">            torch.cat((proposal, gt_box))</span><br><span class="line">            <span class="keyword">for</span> proposal, gt_box <span class="keyword">in</span> <span class="built_in">zip</span>(proposals, gt_boxes)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> proposals</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_targets</span>(<span class="params">self, targets</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">Optional</span>[<span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]]) -&gt; <span class="literal">None</span></span></span><br><span class="line">        <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">all</span>([<span class="string">&quot;boxes&quot;</span> <span class="keyword">in</span> t <span class="keyword">for</span> t <span class="keyword">in</span> targets])</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">all</span>([<span class="string">&quot;labels&quot;</span> <span class="keyword">in</span> t <span class="keyword">for</span> t <span class="keyword">in</span> targets])</span><br><span class="line">        <span class="keyword">if</span> self.has_mask():</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">all</span>([<span class="string">&quot;masks&quot;</span> <span class="keyword">in</span> t <span class="keyword">for</span> t <span class="keyword">in</span> targets])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_training_samples</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                                proposals,  <span class="comment"># type: List[Tensor]</span></span></span><br><span class="line"><span class="params">                                targets     <span class="comment"># type: Optional[List[Dict[str, Tensor]]]</span></span></span><br><span class="line"><span class="params">                                </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        self.check_targets(targets)</span><br><span class="line">        <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        dtype = proposals[<span class="number">0</span>].dtype</span><br><span class="line">        device = proposals[<span class="number">0</span>].device</span><br><span class="line"></span><br><span class="line">        gt_boxes = [t[<span class="string">&quot;boxes&quot;</span>].to(dtype) <span class="keyword">for</span> t <span class="keyword">in</span> targets]</span><br><span class="line">        gt_labels = [t[<span class="string">&quot;labels&quot;</span>] <span class="keyword">for</span> t <span class="keyword">in</span> targets]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># append ground-truth bboxes to propos</span></span><br><span class="line">        proposals = self.add_gt_proposals(proposals, gt_boxes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get matching gt indices for each proposal</span></span><br><span class="line">        matched_idxs, labels = self.assign_targets_to_proposals(proposals, gt_boxes, gt_labels)</span><br><span class="line">        <span class="comment"># sample a fixed proportion of positive-negative proposals</span></span><br><span class="line">        sampled_inds = self.subsample(labels)</span><br><span class="line">        matched_gt_boxes = []</span><br><span class="line">        num_images = <span class="built_in">len</span>(proposals)</span><br><span class="line">        <span class="keyword">for</span> img_id <span class="keyword">in</span> <span class="built_in">range</span>(num_images):</span><br><span class="line">            img_sampled_inds = sampled_inds[img_id]</span><br><span class="line">            proposals[img_id] = proposals[img_id][img_sampled_inds]</span><br><span class="line">            labels[img_id] = labels[img_id][img_sampled_inds]</span><br><span class="line">            matched_idxs[img_id] = matched_idxs[img_id][img_sampled_inds]</span><br><span class="line"></span><br><span class="line">            gt_boxes_in_image = gt_boxes[img_id]</span><br><span class="line">            <span class="keyword">if</span> gt_boxes_in_image.numel() == <span class="number">0</span>:</span><br><span class="line">                gt_boxes_in_image = torch.zeros((<span class="number">1</span>, <span class="number">4</span>), dtype=dtype, device=device)</span><br><span class="line">            matched_gt_boxes.append(gt_boxes_in_image[matched_idxs[img_id]])</span><br><span class="line"></span><br><span class="line">        regression_targets = self.box_coder.encode(matched_gt_boxes, proposals)</span><br><span class="line">        <span class="keyword">return</span> proposals, matched_idxs, labels, regression_targets</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">postprocess_detections</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                               class_logits,    <span class="comment"># type: Tensor</span></span></span><br><span class="line"><span class="params">                               box_regression,  <span class="comment"># type: Tensor</span></span></span><br><span class="line"><span class="params">                               proposals,       <span class="comment"># type: List[Tensor]</span></span></span><br><span class="line"><span class="params">                               image_shapes     <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                               </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        device = class_logits.device</span><br><span class="line">        num_classes = class_logits.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        boxes_per_image = [boxes_in_image.shape[<span class="number">0</span>] <span class="keyword">for</span> boxes_in_image <span class="keyword">in</span> proposals]</span><br><span class="line">        pred_boxes = self.box_coder.decode(box_regression, proposals)</span><br><span class="line"></span><br><span class="line">        pred_scores = F.softmax(class_logits, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        pred_boxes_list = pred_boxes.split(boxes_per_image, <span class="number">0</span>)</span><br><span class="line">        pred_scores_list = pred_scores.split(boxes_per_image, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        all_boxes = []</span><br><span class="line">        all_scores = []</span><br><span class="line">        all_labels = []</span><br><span class="line">        <span class="keyword">for</span> boxes, scores, image_shape <span class="keyword">in</span> <span class="built_in">zip</span>(pred_boxes_list, pred_scores_list, image_shapes):</span><br><span class="line">            boxes = box_ops.clip_boxes_to_image(boxes, image_shape)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># create labels for each prediction</span></span><br><span class="line">            labels = torch.arange(num_classes, device=device)</span><br><span class="line">            labels = labels.view(<span class="number">1</span>, -<span class="number">1</span>).expand_as(scores)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove predictions with the background label</span></span><br><span class="line">            boxes = boxes[:, <span class="number">1</span>:]</span><br><span class="line">            scores = scores[:, <span class="number">1</span>:]</span><br><span class="line">            labels = labels[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># batch everything, by making every class prediction be a separate instance</span></span><br><span class="line">            boxes = boxes.reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">            scores = scores.reshape(-<span class="number">1</span>)</span><br><span class="line">            labels = labels.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove low scoring boxes</span></span><br><span class="line">            inds = torch.where(scores &gt; self.score_thresh)[<span class="number">0</span>]</span><br><span class="line">            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove empty boxes</span></span><br><span class="line">            keep = box_ops.remove_small_boxes(boxes, min_size=<span class="number">1e-2</span>)</span><br><span class="line">            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># non-maximum suppression, independently done per class</span></span><br><span class="line">            keep = box_ops.batched_nms(boxes, scores, labels, self.nms_thresh)</span><br><span class="line">            <span class="comment"># keep only topk scoring predictions</span></span><br><span class="line">            keep = keep[:self.detections_per_img]</span><br><span class="line">            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]</span><br><span class="line"></span><br><span class="line">            all_boxes.append(boxes)</span><br><span class="line">            all_scores.append(scores)</span><br><span class="line">            all_labels.append(labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_boxes, all_scores, all_labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                features,      <span class="comment"># type: Dict[str, Tensor]</span></span></span><br><span class="line"><span class="params">                proposals,     <span class="comment"># type: List[Tensor]</span></span></span><br><span class="line"><span class="params">                image_shapes,  <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                targets=<span class="literal">None</span>   <span class="comment"># type: Optional[List[Dict[str, Tensor]]]</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]], <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            features (List[Tensor])</span></span><br><span class="line"><span class="string">            proposals (List[Tensor[N, 4]])</span></span><br><span class="line"><span class="string">            image_shapes (List[Tuple[H, W]])</span></span><br><span class="line"><span class="string">            targets (List[Dict])</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">                <span class="comment"># <span class="doctag">TODO:</span> https://github.com/pytorch/pytorch/issues/26731</span></span><br><span class="line">                floating_point_types = (torch.<span class="built_in">float</span>, torch.double, torch.half)</span><br><span class="line">                <span class="keyword">assert</span> t[<span class="string">&quot;boxes&quot;</span>].dtype <span class="keyword">in</span> floating_point_types, <span class="string">&#x27;target boxes must of float type&#x27;</span></span><br><span class="line">                <span class="keyword">assert</span> t[<span class="string">&quot;labels&quot;</span>].dtype == torch.int64, <span class="string">&#x27;target labels must of int64 type&#x27;</span></span><br><span class="line">                <span class="keyword">if</span> self.has_keypoint():</span><br><span class="line">                    <span class="keyword">assert</span> t[<span class="string">&quot;keypoints&quot;</span>].dtype == torch.float32, <span class="string">&#x27;target keypoints must of float type&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels = <span class="literal">None</span></span><br><span class="line">            regression_targets = <span class="literal">None</span></span><br><span class="line">            matched_idxs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        box_features = self.box_roi_pool(features, proposals, image_shapes)</span><br><span class="line">        box_features = self.box_head(box_features)</span><br><span class="line">        class_logits, box_regression = self.box_predictor(box_features)</span><br><span class="line"></span><br><span class="line">        result = torch.jit.annotate(<span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]], [])</span><br><span class="line">        losses = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">assert</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> regression_targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            loss_classifier, loss_box_reg = fastrcnn_loss(</span><br><span class="line">                class_logits, box_regression, labels, regression_targets)</span><br><span class="line">            losses = &#123;</span><br><span class="line">                <span class="string">&quot;loss_classifier&quot;</span>: loss_classifier,</span><br><span class="line">                <span class="string">&quot;loss_box_reg&quot;</span>: loss_box_reg</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)</span><br><span class="line">            num_images = <span class="built_in">len</span>(boxes)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_images):</span><br><span class="line">                result.append(</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="string">&quot;boxes&quot;</span>: boxes[i],</span><br><span class="line">                        <span class="string">&quot;labels&quot;</span>: labels[i],</span><br><span class="line">                        <span class="string">&quot;scores&quot;</span>: scores[i],</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.has_mask():</span><br><span class="line">            mask_proposals = [p[<span class="string">&quot;boxes&quot;</span>] <span class="keyword">for</span> p <span class="keyword">in</span> result]</span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="keyword">assert</span> matched_idxs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                <span class="comment"># during training, only focus on positive boxes</span></span><br><span class="line">                num_images = <span class="built_in">len</span>(proposals)</span><br><span class="line">                mask_proposals = []</span><br><span class="line">                pos_matched_idxs = []</span><br><span class="line">                <span class="keyword">for</span> img_id <span class="keyword">in</span> <span class="built_in">range</span>(num_images):</span><br><span class="line">                    pos = torch.where(labels[img_id] &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">                    mask_proposals.append(proposals[img_id][pos])</span><br><span class="line">                    pos_matched_idxs.append(matched_idxs[img_id][pos])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos_matched_idxs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.mask_roi_pool <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)</span><br><span class="line">                mask_features = self.mask_head(mask_features)</span><br><span class="line">                mask_logits = self.mask_predictor(mask_features)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                mask_logits = torch.tensor(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">&quot;Expected mask_roi_pool to be not None&quot;</span>)</span><br><span class="line"></span><br><span class="line">            loss_mask = &#123;&#125;</span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                <span class="keyword">assert</span> pos_matched_idxs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                <span class="keyword">assert</span> mask_logits <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                gt_masks = [t[<span class="string">&quot;masks&quot;</span>] <span class="keyword">for</span> t <span class="keyword">in</span> targets]</span><br><span class="line">                gt_labels = [t[<span class="string">&quot;labels&quot;</span>] <span class="keyword">for</span> t <span class="keyword">in</span> targets]</span><br><span class="line">                rcnn_loss_mask = maskrcnn_loss(</span><br><span class="line">                    mask_logits, mask_proposals,</span><br><span class="line">                    gt_masks, gt_labels, pos_matched_idxs)</span><br><span class="line">                loss_mask = &#123;</span><br><span class="line">                    <span class="string">&quot;loss_mask&quot;</span>: rcnn_loss_mask</span><br><span class="line">                &#125;</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                labels = [r[<span class="string">&quot;labels&quot;</span>] <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line">                masks_probs = maskrcnn_inference(mask_logits, labels)</span><br><span class="line">                <span class="keyword">for</span> mask_prob, r <span class="keyword">in</span> <span class="built_in">zip</span>(masks_probs, result):</span><br><span class="line">                    r[<span class="string">&quot;masks&quot;</span>] = mask_prob</span><br><span class="line"></span><br><span class="line">            losses.update(loss_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># keep none checks in if conditional so torchscript will conditionally</span></span><br><span class="line">        <span class="comment"># compile each branch</span></span><br><span class="line">        <span class="keyword">if</span> self.keypoint_roi_pool <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> self.keypoint_head <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> \</span><br><span class="line">                <span class="keyword">and</span> self.keypoint_predictor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            keypoint_proposals = [p[<span class="string">&quot;boxes&quot;</span>] <span class="keyword">for</span> p <span class="keyword">in</span> result]</span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="comment"># during training, only focus on positive boxes</span></span><br><span class="line">                num_images = <span class="built_in">len</span>(proposals)</span><br><span class="line">                keypoint_proposals = []</span><br><span class="line">                pos_matched_idxs = []</span><br><span class="line">                <span class="keyword">assert</span> matched_idxs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                <span class="keyword">for</span> img_id <span class="keyword">in</span> <span class="built_in">range</span>(num_images):</span><br><span class="line">                    pos = torch.where(labels[img_id] &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">                    keypoint_proposals.append(proposals[img_id][pos])</span><br><span class="line">                    pos_matched_idxs.append(matched_idxs[img_id][pos])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos_matched_idxs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            keypoint_features = self.keypoint_roi_pool(features, keypoint_proposals, image_shapes)</span><br><span class="line">            keypoint_features = self.keypoint_head(keypoint_features)</span><br><span class="line">            keypoint_logits = self.keypoint_predictor(keypoint_features)</span><br><span class="line"></span><br><span class="line">            loss_keypoint = &#123;&#125;</span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                <span class="keyword">assert</span> pos_matched_idxs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                gt_keypoints = [t[<span class="string">&quot;keypoints&quot;</span>] <span class="keyword">for</span> t <span class="keyword">in</span> targets]</span><br><span class="line">                rcnn_loss_keypoint = keypointrcnn_loss(</span><br><span class="line">                    keypoint_logits, keypoint_proposals,</span><br><span class="line">                    gt_keypoints, pos_matched_idxs)</span><br><span class="line">                loss_keypoint = &#123;</span><br><span class="line">                    <span class="string">&quot;loss_keypoint&quot;</span>: rcnn_loss_keypoint</span><br><span class="line">                &#125;</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">assert</span> keypoint_logits <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                <span class="keyword">assert</span> keypoint_proposals <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                keypoints_probs, kp_scores = keypointrcnn_inference(keypoint_logits, keypoint_proposals)</span><br><span class="line">                <span class="keyword">for</span> keypoint_prob, kps, r <span class="keyword">in</span> <span class="built_in">zip</span>(keypoints_probs, kp_scores, result):</span><br><span class="line">                    r[<span class="string">&quot;keypoints&quot;</span>] = keypoint_prob</span><br><span class="line">                    r[<span class="string">&quot;keypoints_scores&quot;</span>] = kps</span><br><span class="line"></span><br><span class="line">            losses.update(loss_keypoint)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result, losses</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>主要看<code>__forward__</code>函数的实现，虽然很长，但是如果只考虑Faster
R-CNN需要的部分（不考虑用于Mask
R-CNN的图像分割分支），其实可以概括为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            features,      <span class="comment"># type: Dict[str, Tensor]</span></span></span><br><span class="line"><span class="params">            proposals,     <span class="comment"># type: List[Tensor]</span></span></span><br><span class="line"><span class="params">            image_shapes,  <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">            targets=<span class="literal">None</span>   <span class="comment"># type: Optional[List[Dict[str, Tensor]]]</span></span></span><br><span class="line"><span class="params">            </span>):</span><br><span class="line">    <span class="comment"># 1. RoI Pool (or RoI Align)</span></span><br><span class="line">    box_features = self.box_roi_pool(features, proposals, image_shapes)</span><br><span class="line">    <span class="comment"># 2. MLP Head</span></span><br><span class="line">    box_features = self.box_head(box_features)</span><br><span class="line">    <span class="comment"># 3. Predictor</span></span><br><span class="line">    class_logits, box_regression = self.box_predictor(box_features)</span><br><span class="line"></span><br><span class="line">    result = torch.jit.annotate(<span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]], [])</span><br><span class="line">    losses = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> self.training:</span><br><span class="line">        <span class="keyword">assert</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> regression_targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        loss_classifier, loss_box_reg = fastrcnn_loss(</span><br><span class="line">            class_logits, box_regression, labels, regression_targets)</span><br><span class="line">        losses = &#123;</span><br><span class="line">            <span class="string">&quot;loss_classifier&quot;</span>: loss_classifier,</span><br><span class="line">            <span class="string">&quot;loss_box_reg&quot;</span>: loss_box_reg</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 4. Postprocess Detections</span></span><br><span class="line">        boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)</span><br><span class="line">        num_images = <span class="built_in">len</span>(boxes)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_images):</span><br><span class="line">            result.append(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;boxes&quot;</span>: boxes[i],</span><br><span class="line">                    <span class="string">&quot;labels&quot;</span>: labels[i],</span><br><span class="line">                    <span class="string">&quot;scores&quot;</span>: scores[i],</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result, losses</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Faster R-CNN的RoIHeads主要包含几个步骤：</p>
<ol type="1">
<li>RoI
Pool：由<code>box_features = self.box_roi_pool(features, proposals, image_shapes)</code>执行，Faster
R-CNN的RoI
Pool的具体实现是<code>torchvision.ops.poolers</code>包中的<code>MultiScaleRoIAlign</code>类。因为目标的形状不尽相同，所以涉及到的特征窗口就不尽相同。RoI
Pool的目的在于通过把尺寸不定的RoI
window划分为固定的网格做池化，来把输入的变长的RoI特征池化为定长的特征输出，方便后续的特征处理。</li>
<li>MLP
Head：由<code>box_features = self.box_head(box_features)</code>执行，Faster
R-CNN的MLP
Head的具体实现是<code>torchvision.models.detection.faster_rcnn</code>中的<code>TwoMLPHead</code>类。MLP
Head承接RoI
Pool池化出的定长特征向量，并通过MLP做非线性计算，输出最终特征用于后续的任务（分类、回归等）。</li>
<li>Predictor：由<code>class_logits, box_regression = self.box_predictor(box_features)</code>执行，Faster
R-CNN的Predictor的具体实现是<code>torchvision.models.detection.faster_rcnn</code>中的<code>FastRCNNPredictor</code>类。上一步MLP操作输出的特征作为最后的特征，交给Predictor去做具体任务的预测，例如：目标分类，bbox位置和尺寸值的回归预测。</li>
<li>Postprocess
Detections：由<code>boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)</code>，该函数是RoIHeads类的一个成员函数。</li>
</ol>
<h5 id="multiscaleroialign">MultiScaleRoIAlign</h5>
<p><code>torchvision</code>采用<code>torchvision.ops.poolers</code>包中的<code>MultiScaleRoIAlign</code>作为Faster
R-CNN的RoI Pool的实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiScaleRoIAlign</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Multi-scale RoIAlign pooling, which is useful for detection with or without FPN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It infers the scale of the pooling via the heuristics present in the FPN paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        featmap_names (List[str]): the names of the feature maps that will be used</span></span><br><span class="line"><span class="string">            for the pooling.</span></span><br><span class="line"><span class="string">        output_size (List[Tuple[int, int]] or List[int]): output size for the pooled region</span></span><br><span class="line"><span class="string">        sampling_ratio (int): sampling ratio for ROIAlign</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; m = torchvision.ops.MultiScaleRoIAlign([&#x27;feat1&#x27;, &#x27;feat3&#x27;], 3, 2)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; i = OrderedDict()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; i[&#x27;feat1&#x27;] = torch.rand(1, 5, 64, 64)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; i[&#x27;feat2&#x27;] = torch.rand(1, 5, 32, 32)  # this feature won&#x27;t be used in the pooling</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; i[&#x27;feat3&#x27;] = torch.rand(1, 5, 16, 16)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # create some random bounding boxes</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # original image size, before computing the feature maps</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; image_sizes = [(512, 512)]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = m(i, [boxes], image_sizes)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(output.shape)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; torch.Size([6, 5, 3, 3])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    __annotations__ = &#123;</span><br><span class="line">        <span class="string">&#x27;scales&#x27;</span>: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]],</span><br><span class="line">        <span class="string">&#x27;map_levels&#x27;</span>: <span class="type">Optional</span>[LevelMapper]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        featmap_names: <span class="type">List</span>[<span class="built_in">str</span>],</span></span><br><span class="line"><span class="params">        output_size: <span class="type">Union</span>[<span class="built_in">int</span>, <span class="type">Tuple</span>[<span class="built_in">int</span>], <span class="type">List</span>[<span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">        sampling_ratio: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiScaleRoIAlign, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(output_size, <span class="built_in">int</span>):</span><br><span class="line">            output_size = (output_size, output_size)</span><br><span class="line">        self.featmap_names = featmap_names</span><br><span class="line">        self.sampling_ratio = sampling_ratio</span><br><span class="line">        self.output_size = <span class="built_in">tuple</span>(output_size)</span><br><span class="line">        self.scales = <span class="literal">None</span></span><br><span class="line">        self.map_levels = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert_to_roi_format</span>(<span class="params">self, boxes: <span class="type">List</span>[Tensor]</span>) -&gt; Tensor:</span><br><span class="line">        concat_boxes = torch.cat(boxes, dim=<span class="number">0</span>)</span><br><span class="line">        device, dtype = concat_boxes.device, concat_boxes.dtype</span><br><span class="line">        ids = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                torch.full_like(b[:, :<span class="number">1</span>], i, dtype=dtype, layout=torch.strided, device=device)</span><br><span class="line">                <span class="keyword">for</span> i, b <span class="keyword">in</span> <span class="built_in">enumerate</span>(boxes)</span><br><span class="line">            ],</span><br><span class="line">            dim=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        rois = torch.cat([ids, concat_boxes], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> rois</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">infer_scale</span>(<span class="params">self, feature: Tensor, original_size: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="comment"># assumption: the scale is of the form 2 ** (-k), with k integer</span></span><br><span class="line">        size = feature.shape[-<span class="number">2</span>:]</span><br><span class="line">        possible_scales = torch.jit.annotate(<span class="type">List</span>[<span class="built_in">float</span>], [])</span><br><span class="line">        <span class="keyword">for</span> s1, s2 <span class="keyword">in</span> <span class="built_in">zip</span>(size, original_size):</span><br><span class="line">            approx_scale = <span class="built_in">float</span>(s1) / <span class="built_in">float</span>(s2)</span><br><span class="line">            scale = <span class="number">2</span> ** <span class="built_in">float</span>(torch.tensor(approx_scale).log2().<span class="built_in">round</span>())</span><br><span class="line">            possible_scales.append(scale)</span><br><span class="line">        <span class="keyword">assert</span> possible_scales[<span class="number">0</span>] == possible_scales[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> possible_scales[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup_scales</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        features: <span class="type">List</span>[Tensor],</span></span><br><span class="line"><span class="params">        image_shapes: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(image_shapes) != <span class="number">0</span></span><br><span class="line">        max_x = <span class="number">0</span></span><br><span class="line">        max_y = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> shape <span class="keyword">in</span> image_shapes:</span><br><span class="line">            max_x = <span class="built_in">max</span>(shape[<span class="number">0</span>], max_x)</span><br><span class="line">            max_y = <span class="built_in">max</span>(shape[<span class="number">1</span>], max_y)</span><br><span class="line">        original_input_shape = (max_x, max_y)</span><br><span class="line"></span><br><span class="line">        scales = [self.infer_scale(feat, original_input_shape) <span class="keyword">for</span> feat <span class="keyword">in</span> features]</span><br><span class="line">        <span class="comment"># get the levels in the feature map by leveraging the fact that the network always</span></span><br><span class="line">        <span class="comment"># downsamples by a factor of 2 at each level.</span></span><br><span class="line">        lvl_min = -torch.log2(torch.tensor(scales[<span class="number">0</span>], dtype=torch.float32)).item()</span><br><span class="line">        lvl_max = -torch.log2(torch.tensor(scales[-<span class="number">1</span>], dtype=torch.float32)).item()</span><br><span class="line">        self.scales = scales</span><br><span class="line">        self.map_levels = initLevelMapper(<span class="built_in">int</span>(lvl_min), <span class="built_in">int</span>(lvl_max))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: <span class="type">Dict</span>[<span class="built_in">str</span>, Tensor],</span></span><br><span class="line"><span class="params">        boxes: <span class="type">List</span>[Tensor],</span></span><br><span class="line"><span class="params">        image_shapes: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">    </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            x (OrderedDict[Tensor]): feature maps for each level. They are assumed to have</span></span><br><span class="line"><span class="string">                all the same number of channels, but they can have different sizes.</span></span><br><span class="line"><span class="string">            boxes (List[Tensor[N, 4]]): boxes to be used to perform the pooling operation, in</span></span><br><span class="line"><span class="string">                (x1, y1, x2, y2) format and in the image reference size, not the feature map</span></span><br><span class="line"><span class="string">                reference.</span></span><br><span class="line"><span class="string">            image_shapes (List[Tuple[height, width]]): the sizes of each image before they</span></span><br><span class="line"><span class="string">                have been fed to a CNN to obtain feature maps. This allows us to infer the</span></span><br><span class="line"><span class="string">                scale factor for each one of the levels to be pooled.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            result (Tensor)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x_filtered = []</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> x.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">in</span> self.featmap_names:</span><br><span class="line">                x_filtered.append(v)</span><br><span class="line">        num_levels = <span class="built_in">len</span>(x_filtered)</span><br><span class="line">        rois = self.convert_to_roi_format(boxes)</span><br><span class="line">        <span class="keyword">if</span> self.scales <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.setup_scales(x_filtered, image_shapes)</span><br><span class="line"></span><br><span class="line">        scales = self.scales</span><br><span class="line">        <span class="keyword">assert</span> scales <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_levels == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> roi_align(</span><br><span class="line">                x_filtered[<span class="number">0</span>], rois,</span><br><span class="line">                output_size=self.output_size,</span><br><span class="line">                spatial_scale=scales[<span class="number">0</span>],</span><br><span class="line">                sampling_ratio=self.sampling_ratio</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        mapper = self.map_levels</span><br><span class="line">        <span class="keyword">assert</span> mapper <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        levels = mapper(boxes)</span><br><span class="line"></span><br><span class="line">        num_rois = <span class="built_in">len</span>(rois)</span><br><span class="line">        num_channels = x_filtered[<span class="number">0</span>].shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        dtype, device = x_filtered[<span class="number">0</span>].dtype, x_filtered[<span class="number">0</span>].device</span><br><span class="line">        result = torch.zeros(</span><br><span class="line">            (num_rois, num_channels,) + self.output_size,</span><br><span class="line">            dtype=dtype,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        tracing_results = []</span><br><span class="line">        <span class="keyword">for</span> level, (per_level_feature, scale) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x_filtered, scales)):</span><br><span class="line">            idx_in_level = torch.where(levels == level)[<span class="number">0</span>]</span><br><span class="line">            rois_per_level = rois[idx_in_level]</span><br><span class="line"></span><br><span class="line">            result_idx_in_level = roi_align(</span><br><span class="line">                per_level_feature, rois_per_level,</span><br><span class="line">                output_size=self.output_size,</span><br><span class="line">                spatial_scale=scale, sampling_ratio=self.sampling_ratio)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">                tracing_results.append(result_idx_in_level.to(dtype))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># result and result_idx_in_level&#x27;s dtypes are based on dtypes of different</span></span><br><span class="line">                <span class="comment"># elements in x_filtered.  x_filtered contains tensors output by different</span></span><br><span class="line">                <span class="comment"># layers.  When autocast is active, it may choose different dtypes for</span></span><br><span class="line">                <span class="comment"># different layers&#x27; outputs.  Therefore, we defensively match result&#x27;s dtype</span></span><br><span class="line">                <span class="comment"># before copying elements from result_idx_in_level in the following op.</span></span><br><span class="line">                <span class="comment"># We need to cast manually (can&#x27;t rely on autocast to cast for us) because</span></span><br><span class="line">                <span class="comment"># the op acts on result in-place, and autocast only affects out-of-place ops.</span></span><br><span class="line">                result[idx_in_level] = result_idx_in_level.to(result.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">            result = _onnx_merge_levels(levels, tracing_results)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>实际上Faster R-CNN论文发表时并没有RoI Align技术，当时仍然沿用的是Fast
R-CNN中的RoI Pool。RoI
Pool指的是对RoI内的特征做池化，取得一个小的feature
map，即，把原来形状不定的<span class="math inline">\(h \times
w\)</span>（<span class="math inline">\(h,
w\)</span>均为变量）的RoI窗口内的特征池化为统一的<span
class="math inline">\(H \times W\)</span>（<span
class="math inline">\(H, W\)</span>均为常量）的小feature map。</p>
<p>RoI Align其实是Mask R-CNN论文中提出的概念。RoI Align觉得RoI
Pool的处理太粗糙了，存在量化（Quantization）的问题，计算feature
map上的窗口坐标的时候就舍入取整了，窗口内划分bins的时候又舍入取整了，这样就很不精确。这样的量化处理，用作分类任务倒还影响不大，但是用作图像分割这种像素级精度的任务时就是个问题了。</p>
<blockquote>
<p>RoI Align对RoI
Pool的改进及其二次插值的数学计算原理可以仔细阅读这篇文章：</p>
<p><a
target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193">Understanding
Region of Interest — (RoI Align and RoI Warp) | by Kemal Erdem
(burnpiro) | Towards Data Science</a></p>
</blockquote>
<p>MultiScaleRoIAlign核心的RoI
Align操作是通过调用<code>torchvision.ops.roi_align</code>包的<code>roi_align</code>函数实现的，而该函数实际上也只是执行了对底层<code>torch.ops.torchvision.roi_align</code>函数的调用。</p>
<p>在本例中，输入的两张图片经过RPN处理后，各得到1000个boxes，即共2000个boxes。经过RoIPool
/ RoIAlign处理后，输出为：</p>
<ul>
<li><code>box_features</code>（<code>results</code>）：shape[2000, 256,
7, 7]</li>
</ul>
<p>表示2000个boxes，都被池化为了<span class="math inline">\(C, H, W =
256, 7, 7\)</span>的特征。</p>
<h5 id="twomlphead">TwoMLPHead</h5>
<p><code>torchvision</code>采用<code>torchvision.models.detection.faster_rcnn</code>包中的<code>TwoMLPHead</code>作为MLP
Head的实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoMLPHead</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Standard heads for FPN-based models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">        representation_size (int): size of the intermediate representation</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, representation_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoMLPHead, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.fc6 = nn.Linear(in_channels, representation_size)</span><br><span class="line">        self.fc7 = nn.Linear(representation_size, representation_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc6(x))</span><br><span class="line">        x = F.relu(self.fc7(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这部分并不复杂，实际上就是实现了两层的MLP，名为<code>fc6</code>和<code>fc7</code>。</p>
<p>在本例中，RoIAlign输出的<code>box_features</code>原shape[2000, 256,
7, 7]，在TwoMLPHead中：</p>
<ol type="1">
<li>首先经过flatten处理，变为shape[2000, 12544]；</li>
<li>进过双层MLP处理后，变为shape[2000, 1024]。</li>
</ol>
<p>返回的是如上非线性转换后的<code>box_features</code>特征，此时shape[2000,
1024]。</p>
<h5 id="fastrcnnpredictor">FastRCNNPredictor</h5>
<p><code>torchvision</code>采用<code>torchvision.models.detection.faster_rcnn</code>包中的<code>FastRCNNPredictor</code>作为Predictor的实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FastRCNNPredictor</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Standard classification + bounding box regression layers</span></span><br><span class="line"><span class="string">    for Fast R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">        num_classes (int): number of output classes (including background)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(FastRCNNPredictor, self).__init__()</span><br><span class="line">        self.cls_score = nn.Linear(in_channels, num_classes)</span><br><span class="line">        self.bbox_pred = nn.Linear(in_channels, num_classes * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> x.dim() == <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">list</span>(x.shape[<span class="number">2</span>:]) == [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line">        scores = self.cls_score(x)</span><br><span class="line">        bbox_deltas = self.bbox_pred(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, bbox_deltas</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这部分也并不复杂，实际上就是同论文中描述的一样，通过MLP实现了两个预测分支：</p>
<ol type="1">
<li><code>self.cls_score</code>分支预测目标的分类分值<code>scores</code>；</li>
<li><code>self.bbox_pred</code>分支回归目标对应各个分类的目标框回归值。</li>
</ol>
<p>在<code>torchvision</code>的预训练模型中，FastRCNNPredictor的<code>num_classes</code>是91，即能识别含背景在内的91个类。</p>
<p>在本例中，两个分支根据RoIAlign和TwoMLPHead提取出的特征，分别预测输出：</p>
<ol type="1">
<li><code>class_logits</code>（<code>socres</code>）：shape[2000,
91]；</li>
<li><code>box_regression</code>（<code>bbox_deltas</code>）：shape[2000,
364]。</li>
</ol>
<p>意思是输入的2张图片上共2000个框（1000个/图片），这2000个框都做了分类预测，并且为每个类分别计算了目标框的回归修正值。</p>
<h5 id="postprocess_detections">postprocess_detections</h5>
<p>在模型的Predictor完成预测后，还需要做后续的一些处理，该部分的处理在<code>torchvision.models.detection.roi_heads.RoIHeads</code>的<code>postprocess_detections</code>函数中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RoIHeads</span>(torch.nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">postprocess_detections</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                               class_logits,    <span class="comment"># type: Tensor</span></span></span><br><span class="line"><span class="params">                               box_regression,  <span class="comment"># type: Tensor</span></span></span><br><span class="line"><span class="params">                               proposals,       <span class="comment"># type: List[Tensor]</span></span></span><br><span class="line"><span class="params">                               image_shapes     <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                               </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor]]</span></span><br><span class="line">        device = class_logits.device</span><br><span class="line">        num_classes = class_logits.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        boxes_per_image = [boxes_in_image.shape[<span class="number">0</span>] <span class="keyword">for</span> boxes_in_image <span class="keyword">in</span> proposals]</span><br><span class="line">        pred_boxes = self.box_coder.decode(box_regression, proposals)</span><br><span class="line"></span><br><span class="line">        pred_scores = F.softmax(class_logits, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        pred_boxes_list = pred_boxes.split(boxes_per_image, <span class="number">0</span>)</span><br><span class="line">        pred_scores_list = pred_scores.split(boxes_per_image, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        all_boxes = []</span><br><span class="line">        all_scores = []</span><br><span class="line">        all_labels = []</span><br><span class="line">        <span class="keyword">for</span> boxes, scores, image_shape <span class="keyword">in</span> <span class="built_in">zip</span>(pred_boxes_list, pred_scores_list, image_shapes):</span><br><span class="line">            boxes = box_ops.clip_boxes_to_image(boxes, image_shape)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># create labels for each prediction</span></span><br><span class="line">            labels = torch.arange(num_classes, device=device)</span><br><span class="line">            labels = labels.view(<span class="number">1</span>, -<span class="number">1</span>).expand_as(scores)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove predictions with the background label</span></span><br><span class="line">            boxes = boxes[:, <span class="number">1</span>:]</span><br><span class="line">            scores = scores[:, <span class="number">1</span>:]</span><br><span class="line">            labels = labels[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># batch everything, by making every class prediction be a separate instance</span></span><br><span class="line">            boxes = boxes.reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">            scores = scores.reshape(-<span class="number">1</span>)</span><br><span class="line">            labels = labels.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove low scoring boxes</span></span><br><span class="line">            inds = torch.where(scores &gt; self.score_thresh)[<span class="number">0</span>]</span><br><span class="line">            boxes, scores, labels = boxes[inds], scores[inds], labels[inds]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove empty boxes</span></span><br><span class="line">            keep = box_ops.remove_small_boxes(boxes, min_size=<span class="number">1e-2</span>)</span><br><span class="line">            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># non-maximum suppression, independently done per class</span></span><br><span class="line">            keep = box_ops.batched_nms(boxes, scores, labels, self.nms_thresh)</span><br><span class="line">            <span class="comment"># keep only topk scoring predictions</span></span><br><span class="line">            keep = keep[:self.detections_per_img]</span><br><span class="line">            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]</span><br><span class="line"></span><br><span class="line">            all_boxes.append(boxes)</span><br><span class="line">            all_scores.append(scores)</span><br><span class="line">            all_labels.append(labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_boxes, all_scores, all_labels</span><br></pre></td></tr></table></figure>
<p>后处理在for循环中，遍历每一张图片：</p>
<ol type="1">
<li>去除背景类的框；</li>
<li>去除低分框；</li>
<li>去除空框（尺寸极小的无意义小框）；</li>
<li>对它的boxes和scores做（类内）NMS；</li>
<li>保留<code>self.detections_per_img</code>个的top-k个目标。</li>
</ol>
<p>因为本例输入的是随机值填充的模拟图片，所以在去除低分框的环节，2000个候选框就因为没有实际的目标而被全部滤除了。</p>
<h4
id="generalizedrcnntransform.postprocess">GeneralizedRCNNTransform.postprocess</h4>
<p>Faster
R-CNN模型的后期处理由<code>torchvision.models.detection.transform</code>包的<code>GeneralizedRCNNTransform</code>类实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GeneralizedRCNNTransform</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                    result,               <span class="comment"># type: List[Dict[str, Tensor]]</span></span></span><br><span class="line"><span class="params">                    image_shapes,         <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                    original_image_sizes  <span class="comment"># type: List[Tuple[int, int]]</span></span></span><br><span class="line"><span class="params">                    </span>):</span><br><span class="line">        <span class="comment"># type: (...) -&gt; <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]]</span></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">for</span> i, (pred, im_s, o_im_s) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(result, image_shapes, original_image_sizes)):</span><br><span class="line">            boxes = pred[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line">            boxes = resize_boxes(boxes, im_s, o_im_s)</span><br><span class="line">            result[i][<span class="string">&quot;boxes&quot;</span>] = boxes</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;masks&quot;</span> <span class="keyword">in</span> pred:</span><br><span class="line">                masks = pred[<span class="string">&quot;masks&quot;</span>]</span><br><span class="line">                masks = paste_masks_in_image(masks, boxes, o_im_s)</span><br><span class="line">                result[i][<span class="string">&quot;masks&quot;</span>] = masks</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;keypoints&quot;</span> <span class="keyword">in</span> pred:</span><br><span class="line">                keypoints = pred[<span class="string">&quot;keypoints&quot;</span>]</span><br><span class="line">                keypoints = resize_keypoints(keypoints, im_s, o_im_s)</span><br><span class="line">                result[i][<span class="string">&quot;keypoints&quot;</span>] = keypoints</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>其实对于目标检测而言，实际上只对<code>boxes</code>的坐标做了resize的操作。因为<code>GeneralizedRCNNTransform</code>在对输入图像做预处理的时候，有进行尺寸转换，而且转tensor的时候又增加了padding是同一batch的图像张量能够保持尺寸一致。所以输出结果的时候，还是要把在tensor上的坐标转换为原始图像尺度上的坐标。</p>
<h2 id="总结">3 总结</h2>
<p>最后总览一下整个模型的实现结构，只需通过简单的<code>print</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<p>，即可输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br></pre></td><td class="code"><pre><span class="line">FasterRCNN(</span><br><span class="line">  (transform): GeneralizedRCNNTransform(</span><br><span class="line">      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</span><br><span class="line">      Resize(min_size=(800,), max_size=1333, mode=&#x27;bilinear&#x27;)</span><br><span class="line">  )</span><br><span class="line">  (backbone): BackboneWithFPN(</span><br><span class="line">    (body): IntermediateLayerGetter(</span><br><span class="line">      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">      (bn1): FrozenBatchNorm2d(64)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">      (layer1): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(64)</span><br><span class="line">          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(64)</span><br><span class="line">          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(256)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">          (downsample): Sequential(</span><br><span class="line">            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">            (1): FrozenBatchNorm2d(256)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(64)</span><br><span class="line">          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(64)</span><br><span class="line">          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(256)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(64)</span><br><span class="line">          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(64)</span><br><span class="line">          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(256)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (layer2): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(512)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">          (downsample): Sequential(</span><br><span class="line">            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">            (1): FrozenBatchNorm2d(512)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(512)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(512)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (3): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(128)</span><br><span class="line">          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(512)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (layer3): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(1024)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">          (downsample): Sequential(</span><br><span class="line">            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">            (1): FrozenBatchNorm2d(1024)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(1024)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(1024)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (3): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(1024)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (4): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(1024)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (5): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(256)</span><br><span class="line">          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(1024)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (layer4): Sequential(</span><br><span class="line">        (0): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(512)</span><br><span class="line">          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(512)</span><br><span class="line">          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(2048)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">          (downsample): Sequential(</span><br><span class="line">            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">            (1): FrozenBatchNorm2d(2048)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (1): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(512)</span><br><span class="line">          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(512)</span><br><span class="line">          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(2048)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">        (2): Bottleneck(</span><br><span class="line">          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn1): FrozenBatchNorm2d(512)</span><br><span class="line">          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">          (bn2): FrozenBatchNorm2d(512)</span><br><span class="line">          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">          (bn3): FrozenBatchNorm2d(2048)</span><br><span class="line">          (relu): ReLU(inplace=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (fpn): FeaturePyramidNetwork(</span><br><span class="line">      (inner_blocks): ModuleList(</span><br><span class="line">        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      )</span><br><span class="line">      (layer_blocks): ModuleList(</span><br><span class="line">        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      )</span><br><span class="line">      (extra_blocks): LastLevelMaxPool()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (rpn): RegionProposalNetwork(</span><br><span class="line">    (anchor_generator): AnchorGenerator()</span><br><span class="line">    (head): RPNHead(</span><br><span class="line">      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (roi_heads): RoIHeads(</span><br><span class="line">    (box_roi_pool): MultiScaleRoIAlign()</span><br><span class="line">    (box_head): TwoMLPHead(</span><br><span class="line">      (fc6): Linear(in_features=12544, out_features=1024, bias=True)</span><br><span class="line">      (fc7): Linear(in_features=1024, out_features=1024, bias=True)</span><br><span class="line">    )</span><br><span class="line">    (box_predictor): FastRCNNPredictor(</span><br><span class="line">      (cls_score): Linear(in_features=1024, out_features=91, bias=True)</span><br><span class="line">      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>模型结构可以总结为层次结构：</p>
<ul>
<li>Faster R-CNN
<ul>
<li>(transform): GeneralizedRCNNTransform</li>
<li>(backbone): BackboneWithFPN
<ul>
<li>(body): IntermediateLayerGetter</li>
<li>(fpn): FeaturePyramidNetwork</li>
</ul></li>
<li>(rpn): RegionProposalNetwork
<ul>
<li>(anchor_generator): AnchorGenerator</li>
<li>(head): RPNHead</li>
</ul></li>
<li>(roi_heads): RoIHeads
<ul>
<li>(box_roi_pool): MultiScaleRoIAlign</li>
<li>(box_head): TwoMLPHead</li>
<li>(box_predictor): FastRCNNPredictor</li>
</ul></li>
</ul></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>支持我的写作！(Support my writing!)</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="Heary 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Heary 支付宝">
        <span>支付宝</span>
      </div>
      <div>
        <img src="/images/paypal.png" alt="Heary PayPal">
        <span>PayPal</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Heary
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://heary.cn/posts/fasterrcnn-resnet50-fpn-%E4%BB%8Etorchvision%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3Faster-R-CNN%E5%8E%9F%E7%90%86/" title="fasterrcnn_resnet50_fpn - 从torchvision源码理解Faster R-CNN原理">https://heary.cn/posts/fasterrcnn-resnet50-fpn-从torchvision源码理解Faster-R-CNN原理/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/R-CNN/" rel="tag"><i class="fa fa-tag"></i> R-CNN</a>
              <a href="/tags/Object-Detection/" rel="tag"><i class="fa fa-tag"></i> Object Detection</a>
              <a href="/tags/CV/" rel="tag"><i class="fa fa-tag"></i> CV</a>
              <a href="/tags/PyTorch/" rel="tag"><i class="fa fa-tag"></i> PyTorch</a>
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
              <a href="/tags/Faster-R-CNN/" rel="tag"><i class="fa fa-tag"></i> Faster R-CNN</a>
              <a href="/tags/torchvision/" rel="tag"><i class="fa fa-tag"></i> torchvision</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/Go%E7%AC%94%E8%AE%B0/" rel="prev" title="Go笔记">
                  <i class="fa fa-chevron-left"></i> Go笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/DistributedDataParallel-DDP-PyTorch%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/" rel="next" title="DistributedDataParallel(DDP) - PyTorch多进程并行计算">
                  DistributedDataParallel(DDP) - PyTorch多进程并行计算 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Heary</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">693k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">10:30</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.2/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"HearyShen","repo":"gitalk-comments","client_id":"77d476596d65dea261b8","client_secret":"225837e23b2eb099dc77a3aa38dbeefffef4599b","admin_user":"HearyShen","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.2/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"081aab58f31f9c829d3e055b5a2fc544"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
