<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="baidu-site-verification" content="WmzZoAIhtw8L5PpX">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"heary.cn","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"post_block":"zoomIn","post_header":"zoomIn","post_body":"fadeInUp","coll_header":"fadeInLeft","sidebar":"zoomIn"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="在实验机器上配置好了CUDA、cuDNN和PyTorch，开始上手PyTorch。持续更新。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch学习笔记">
<meta property="og:url" content="https://heary.cn/posts/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Heary&#39;s Blog">
<meta property="og:description" content="在实验机器上配置好了CUDA、cuDNN和PyTorch，开始上手PyTorch。持续更新。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-03-07T03:53:46.000Z">
<meta property="article:modified_time" content="2022-08-07T04:02:09.613Z">
<meta property="article:author" content="Heary">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://heary.cn/posts/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://heary.cn/posts/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","path":"posts/PyTorch学习笔记/","title":"PyTorch学习笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PyTorch学习笔记 | Heary's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-135282529-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-135282529-1","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?93bca42dda78417b488ac006a6ac5444"></script>


  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;e56a1ace8bf142f3b73bedc96953a959&quot;}'></script>


  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Heary's Blog" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="Heary's Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Heary's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">渡口缀满灯花</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">186</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">146</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><span class="nav-text">PyTorch学习笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90"><span class="nav-text">0 学习资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-text">1 背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#jupyter-notebook"><span class="nav-text">1.1 Jupyter Notebook</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pytorch"><span class="nav-text">1.2 PyTorch</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8Etorch-lua%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">1.2.1 与Torch, Lua的关系</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-text">2 张量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="nav-text">2.1 构建张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-text">2.2 数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97"><span class="nav-text">2.3 基本运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="nav-text">2.4 基本操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8Enumpy%E4%BA%92%E8%BD%AC"><span class="nav-text">2.5 与NumPy互转</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cuda-tensor"><span class="nav-text">2.6 CUDA Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cuda%E5%92%8C.cpu%E6%96%B9%E6%B3%95"><span class="nav-text">2.6.1 .cuda和.cpu方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#to%E6%96%B9%E6%B3%95"><span class="nav-text">2.6.2 .to方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#autograd%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-text">3 Autograd自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E9%87%8F"><span class="nav-text">3.1 标量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F"><span class="nav-text">3.2 向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%81%E7%94%A8autograd"><span class="nav-text">3.3 禁用autograd</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">4 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="nav-text">4.1 定义网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">4.2 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">4.3 反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E6%9D%83%E9%87%8D"><span class="nav-text">4.4 更新权重</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">5 数据加载与预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dataset"><span class="nav-text">5.1 Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataloader"><span class="nav-text">5.2 DataLoader</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torchvision"><span class="nav-text">5.3 torchvision</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#torchvision.datasets"><span class="nav-text">5.3.1 torchvision.datasets</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torchvision.models"><span class="nav-text">5.3.2 torchvision.models</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torchvision.transforms"><span class="nav-text">5.3.3 torchvision.transforms</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E4%BE%8B"><span class="nav-text">6 图像分类实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="nav-text">6.1 处理数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-text">6.2 定义卷积神经网络模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-text">6.3 定义损失函数与优化器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-text">6.4 训练神经网络模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-text">6.5 测试神经网络模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8gpu%E4%B8%8A%E8%AE%AD%E7%BB%83"><span class="nav-text">6.6 在GPU上训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5gpu%E6%94%AF%E6%8C%81"><span class="nav-text">6.6.1 检查GPU支持</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%BD%BD%E5%85%A5cuda"><span class="nav-text">6.6.2 神经网络模型载入CUDA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E8%BD%BD%E5%85%A5cuda"><span class="nav-text">6.6.3 输入数据载入CUDA</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Heary"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Heary</p>
  <div class="site-description" itemprop="description">养天地正气 法古今完人</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">146</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">186</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/HearyShen" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HearyShen" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/jiayun-shen/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;jiayun-shen&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jiayun.shen@foxmail.com" title="E-Mail → mailto:jiayun.shen@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://heary.cn/" title="https:&#x2F;&#x2F;heary.cn">Heary's Blog(Main)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hearyshen.github.io/" title="https:&#x2F;&#x2F;hearyshen.github.io" rel="noopener" target="_blank">Heary's Blog(Mirror)</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://lonelyone.cn/" title="http:&#x2F;&#x2F;lonelyone.cn" rel="noopener" target="_blank">Lonelyone.cn</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://piaoyu.org/" title="http:&#x2F;&#x2F;piaoyu.org" rel="noopener" target="_blank">piaoyu.org</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://transformerswsz.github.io/" title="https:&#x2F;&#x2F;transformerswsz.github.io" rel="noopener" target="_blank">Swift的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://log4me.github.io/" title="https:&#x2F;&#x2F;log4me.github.io&#x2F;" rel="noopener" target="_blank">logme's blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://lossherl.github.io/" title="https:&#x2F;&#x2F;lossherl.github.io&#x2F;" rel="noopener" target="_blank">Sherl's</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

  <a href="https://github.com/HearyShen" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://heary.cn/posts/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Heary">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heary's Blog">
      <meta itemprop="description" content="养天地正气 法古今完人">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="PyTorch学习笔记 | Heary's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch学习笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-07 11:53:46" itemprop="dateCreated datePublished" datetime="2019-03-07T11:53:46+08:00">2019-03-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-08-07 12:02:09" itemprop="dateModified" datetime="2022-08-07T12:02:09+08:00">2022-08-07</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>21k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>在实验机器上配置好了CUDA、cuDNN和PyTorch，开始上手PyTorch。持续更新。</p>
<span id="more"></span>
<h1 id="pytorch学习笔记">PyTorch学习笔记</h1>
<h2 id="学习资源">0 学习资源</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/zergtant/pytorch-handbook">PyTorch
中文手册（pytorch handbook）</a></p>
</blockquote>
<p>该教程内容简洁易读，且支持最新的PyTorch1.0正式版。以下是我学习及梳理的笔记。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">PyTorch master
documentation</a></p>
</blockquote>
<p>PyTorch官方文档</p>
<h2 id="背景">1 背景</h2>
<h3 id="jupyter-notebook">1.1 Jupyter Notebook</h3>
<p>上述的学习资源中包含大量的Jupyter Notebook示例，需要安装Jupyter
Notebook运行。</p>
<p>我安装了流行的科学计算包管理平台Anaconda3，即自带安装了Jupyter
Notebook。</p>
<p>git clone 该教程后，在该教程根目录运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p>即可在弹出的窗口中看到当前教程文件夹内的目录结构，点击打开各个ipynb即可。</p>
<h3 id="pytorch">1.2 PyTorch</h3>
<p>PyTorch实际上就是基于Python的科学计算包，服务于以下两种场景:</p>
<ul>
<li>作为NumPy的替代品，可以使用GPU的强大计算能力（GPU加速的张量计算）</li>
<li>提供最大的灵活性和高速的深度学习研究平台（包含自动求导系统的深度神经网络）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先要引入相关的包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#打印一下版本</span></span><br><span class="line">torch.__version__</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x27;1.0.1&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="与torch-lua的关系">1.2.1 与Torch, Lua的关系</h4>
<p>Torch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好。</p>
<p>Lua是Torch的上层包装。</p>
<p>PyTorch和Torch使用包含所有相同性能的C库：TH, THC, THNN,
THCUNN，并且它们将继续共享这些库。PyTorch和Torch都使用的是相同的底层，只是使用了不同的上层包装语言。</p>
<p>注：LUA虽然快，但是太小众了，所以才会有PyTorch的出现。</p>
<h2 id="张量">2 张量</h2>
<p><strong>张量(Tensor)</strong>是PyTorch里面基础的运算单位,与Numpy的ndarray相同都表示的是一个多维的矩阵。
与ndarray的最大区别就是，PyTorch的Tensor可以在 GPU 上运行，而 numpy 的
ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。</p>
<p>第零阶张量 （r = 0） 为标量 （Scalar），第一阶张量 （r = 1） 为向量
（Vector）， 第二阶张量 （r = 2） 则成为矩阵
（Matrix），第三阶以上的统称为多维张量。</p>
<h3 id="构建张量">2.1 构建张量</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)	<span class="comment"># 5行3列矩阵tensor，未初始化为任何值</span></span><br><span class="line"><span class="comment"># tensor([[3.5573e-09, 6.2618e+22, 4.7428e+30],</span></span><br><span class="line"><span class="comment">#        [5.0778e+31, 1.8936e+23, 7.7151e+31],</span></span><br><span class="line"><span class="comment">#        [2.9514e+29, 5.0850e+31, 7.5338e+28],</span></span><br><span class="line"><span class="comment">#        [1.3556e-19, 1.8037e+28, 1.4229e-08],</span></span><br><span class="line"><span class="comment">#        [6.2618e+22, 4.7428e+30, 5.0778e+31]])</span></span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)	<span class="comment"># 5行3列矩阵tensor，填充值为[0,1)间随机浮点数</span></span><br><span class="line"><span class="comment"># tensor([[0.9764, 0.2893, 0.7636],</span></span><br><span class="line"><span class="comment">#        [0.4031, 0.1581, 0.9893],</span></span><br><span class="line"><span class="comment">#        [0.0134, 0.0472, 0.5166],</span></span><br><span class="line"><span class="comment">#        [0.6273, 0.5841, 0.0128],</span></span><br><span class="line"><span class="comment">#        [0.3264, 0.5328, 0.0250]])</span></span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0.]])</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)	<span class="comment"># 5行3列矩阵tensor，填充值为长整型数0</span></span><br><span class="line"><span class="comment"># tensor([[0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0]])</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])	<span class="comment"># 根据给定数据创建并初始化张量</span></span><br><span class="line"><span class="comment"># tensor([5.5000, 3.0000])</span></span><br><span class="line"></span><br><span class="line">one = torch.ones(<span class="number">2</span>, <span class="number">2</span>)	<span class="comment"># 全1矩阵</span></span><br><span class="line"><span class="comment"># tensor([[1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1.]])</span></span><br><span class="line"></span><br><span class="line">eye=torch.eye(<span class="number">2</span>,<span class="number">2</span>)		<span class="comment"># 单位矩阵（主对角线元素全1，其余全0）</span></span><br><span class="line"><span class="comment"># tensor([[1., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 1.]])</span></span><br><span class="line"></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)	<span class="comment"># new_*方法创建对象</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)	<span class="comment"># 仍为5行3列的tensor对象，但值和类型发生了变化</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#        [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#        [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#        [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#        [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># tensor([[ 0.2814,  1.2299,  1.4216],</span></span><br><span class="line"><span class="comment">#        [-0.2575, -1.0438, -1.2800],</span></span><br><span class="line"><span class="comment">#        [ 1.0894, -0.2307, -1.5454],</span></span><br><span class="line"><span class="comment">#        [-0.1985, -0.7991,  1.7902],</span></span><br><span class="line"><span class="comment">#        [ 0.1705,  0.2637, -0.1507]])</span></span><br><span class="line"></span><br><span class="line">x.size()	<span class="comment"># 输出tensor尺寸</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">x.shape		<span class="comment"># 与size()结果一致</span></span><br><span class="line"><span class="comment"># torch.Size([5, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多维张量</span></span><br><span class="line">y = torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(y.size())		<span class="comment"># torch.Size([2, 3, 4, 5])</span></span><br><span class="line">y</span><br><span class="line"><span class="comment"># tensor([[[[0.9071, 0.0616, 0.0006, 0.6031, 0.0714],</span></span><br><span class="line"><span class="comment">#           [0.6592, 0.9700, 0.0253, 0.0726, 0.5360],</span></span><br><span class="line"><span class="comment">#           [0.5416, 0.1138, 0.9592, 0.6779, 0.6501],</span></span><br><span class="line"><span class="comment">#           [0.0546, 0.8287, 0.7748, 0.4352, 0.9232]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.0730, 0.4228, 0.7407, 0.4099, 0.1482],</span></span><br><span class="line"><span class="comment">#           [0.5408, 0.9156, 0.6554, 0.5787, 0.9775],</span></span><br><span class="line"><span class="comment">#           [0.4262, 0.3644, 0.1993, 0.4143, 0.5757],</span></span><br><span class="line"><span class="comment">#           [0.9307, 0.8839, 0.8462, 0.0933, 0.6688]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.4447, 0.0929, 0.9882, 0.5392, 0.1159],</span></span><br><span class="line"><span class="comment">#           [0.4790, 0.5115, 0.4005, 0.9486, 0.0054],</span></span><br><span class="line"><span class="comment">#           [0.8955, 0.8097, 0.1227, 0.2250, 0.5830],</span></span><br><span class="line"><span class="comment">#           [0.8483, 0.2070, 0.1067, 0.4727, 0.5095]]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[[0.9438, 0.2601, 0.2885, 0.5457, 0.7528],</span></span><br><span class="line"><span class="comment">#           [0.2971, 0.2171, 0.3910, 0.1924, 0.2570],</span></span><br><span class="line"><span class="comment">#           [0.7491, 0.9749, 0.2703, 0.2198, 0.9472],</span></span><br><span class="line"><span class="comment">#           [0.1216, 0.6647, 0.8809, 0.0125, 0.5513]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.0870, 0.6622, 0.7252, 0.4783, 0.0160],</span></span><br><span class="line"><span class="comment">#           [0.7832, 0.6050, 0.7469, 0.7947, 0.8052],</span></span><br><span class="line"><span class="comment">#           [0.1755, 0.4489, 0.0602, 0.8073, 0.3028],</span></span><br><span class="line"><span class="comment">#           [0.9937, 0.6780, 0.9425, 0.0059, 0.0451]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.3851, 0.8742, 0.5932, 0.4899, 0.8354],</span></span><br><span class="line"><span class="comment">#           [0.8577, 0.3705, 0.0229, 0.7097, 0.7557],</span></span><br><span class="line"><span class="comment">#           [0.1505, 0.3527, 0.0843, 0.0088, 0.8741],</span></span><br><span class="line"><span class="comment">#           [0.6041, 0.8797, 0.6189, 0.9495, 0.1479]]]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标量</span></span><br><span class="line">scalar = torch.tensor(<span class="number">3.1415926</span>)</span><br><span class="line"><span class="built_in">print</span>(scalar)		<span class="comment"># tensor(3.1416)</span></span><br><span class="line">scalar.size()		<span class="comment"># torch.Size([])</span></span><br><span class="line">scalar.item()		<span class="comment"># 3.141592502593994</span></span><br></pre></td></tr></table></figure>
<p>注意：</p>
<ul>
<li><code>torch.rand</code>方法是随机化[0,1)区间内的服从均匀分布的浮点数。</li>
<li><code>torch.randn</code>方法是随机化输出零均值、单位方差的服从正态分布的浮点数。</li>
</ul>
<h3 id="数据类型">2.2 数据类型</h3>
<p>Tensor的基本数据类型有五种：</p>
<ul>
<li>32位浮点型：torch.FloatTensor (默认)</li>
<li>64位整型：torch.LongTensor</li>
<li>32位整型：torch.IntTensor</li>
<li>16位整型：torch.ShortTensor</li>
<li>64位浮点型：torch.DoubleTensor</li>
</ul>
<p>除以上数字类型外，还有 byte和char型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.tensor([<span class="number">3.1415926</span>]) </span><br><span class="line"><span class="built_in">print</span>(tensor)	<span class="comment"># tensor([3.1416])</span></span><br><span class="line">tensor.size()	<span class="comment"># torch.Size([1])</span></span><br><span class="line">tensor.item()	<span class="comment"># 3.141592502593994</span></span><br><span class="line"></span><br><span class="line">tensor.long()	<span class="comment"># tensor([3])</span></span><br><span class="line">tensor.half()	<span class="comment"># tensor([3.1406], dtype=torch.float16)</span></span><br><span class="line">tensor.<span class="built_in">int</span>()	<span class="comment"># tensor([3], dtype=torch.int32)</span></span><br><span class="line">tensor.<span class="built_in">float</span>()	<span class="comment"># tensor([3.1416])</span></span><br><span class="line">tensor.short()	<span class="comment"># tensor([3], dtype=torch.int16)</span></span><br><span class="line">tensor.char()	<span class="comment"># tensor([3], dtype=torch.int8)</span></span><br><span class="line">tensor.byte()	<span class="comment"># tensor([3], dtype=torch.uint8)</span></span><br></pre></td></tr></table></figure>
<h3 id="基本运算">2.3 基本运算</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加法</span></span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加法：提供tensor作为add的参数</span></span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加法：v.xxx_(...)会改变原变量v</span></span><br><span class="line">y.add_(x)</span><br><span class="line">x.copy_(y)	<span class="comment"># 复制，会改变x</span></span><br><span class="line">x.t_()		<span class="comment"># 转置，会改变x</span></span><br></pre></td></tr></table></figure>
<h3 id="基本操作">2.4 基本操作</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取</span></span><br><span class="line">x[:,<span class="number">1</span>]		<span class="comment"># 表示矩阵tensor的第1列中所有行元素（从0计数）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换尺寸</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)		<span class="comment"># 类似NumPy的reshape</span></span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>)   <span class="comment"># size -1 从其他维度推断</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数值 Tensor.item函数</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)		<span class="comment"># tensor([-0.2368])</span></span><br><span class="line"><span class="built_in">print</span>(x.item())	<span class="comment"># -0.23680149018764496</span></span><br><span class="line">x = torch.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>,<span class="number">0</span>])			<span class="comment"># tensor(0.4259)</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>,<span class="number">0</span>].item())	<span class="comment"># 0.42591235041618347</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求最大值</span></span><br><span class="line">max_value, max_idx = torch.<span class="built_in">max</span>(x, dim=<span class="number">1</span>)	<span class="comment"># 沿着行取最大值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求和</span></span><br><span class="line">sum_x = torch.<span class="built_in">sum</span>(x, dim=<span class="number">1</span>)					<span class="comment"># 沿着行求和</span></span><br></pre></td></tr></table></figure>
<h3 id="与numpy互转">2.5 与NumPy互转</h3>
<p>注意：<strong>Tensor和numpy对象共享内存</strong>，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Torch Tensor转NumPy Array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)		<span class="comment"># tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line">b = a.numpy()			<span class="comment"># [1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="comment"># 注意！b与a保持绑定关系，如：</span></span><br><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)	<span class="comment"># tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="built_in">print</span>(b)	<span class="comment"># [2. 2. 2. 2. 2.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy Array转Torch Tensor</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"><span class="comment"># 注意！b与a保持绑定关系，如：</span></span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line"><span class="built_in">print</span>(a)	<span class="comment"># [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="built_in">print</span>(b)	<span class="comment"># tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到 NumPy 的转换.</span></span><br></pre></td></tr></table></figure>
<h3 id="cuda-tensor">2.6 CUDA Tensor</h3>
<p>在支持CUDA的NVIDIA GPU设备上，可以在GPU上建立Tensor进行运算。</p>
<h4 id="cuda和.cpu方法">2.6.1 .cuda和.cpu方法</h4>
<p>一般情况下可以使用<code>.cuda</code>方法将tensor移动到GPU，这步操作需要CUDA设备支持。</p>
<p>使用<code>.cpu</code>方法可以把tensor移动到CPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cpu_a=torch.rand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">cpu_a.<span class="built_in">type</span>()	<span class="comment"># &#x27;torch.FloatTensor&#x27;</span></span><br><span class="line"></span><br><span class="line">gpu_a=cpu_a.cuda()		<span class="comment"># .cuda方法移动tensor到GPU</span></span><br><span class="line">gpu_a.<span class="built_in">type</span>()	<span class="comment"># &#x27;torch.cuda.FloatTensor&#x27;</span></span><br><span class="line"></span><br><span class="line">cpu_b=gpu_a.cpu()		<span class="comment"># .cpu方法移动tensor到CPU</span></span><br><span class="line">cpu_b.<span class="built_in">type</span>()	<span class="comment"># &#x27;torch.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="to方法">2.6.2 .to方法</h4>
<p>使用<code>.to</code> 方法 可以将Tensor移动到任何设备中。</p>
<p>例1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># is_available 函数判断是否有cuda可以使用</span></span><br><span class="line"><span class="comment"># ``torch.device``将张量移动到指定的设备中</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># a CUDA 设备对象</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接在GPU上创建张量y</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 或者直接使用``.to(&quot;cuda&quot;)``将张量x移动到cuda中</span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># ``.to`` 也会对变量的类型做更改</span></span><br><span class="line">    <span class="comment"># tensor([0.6132], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line">	<span class="comment"># tensor([0.6132], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>
<p>例2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.cuda.is_available()来确定是否有cuda设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)		<span class="comment"># cuda</span></span><br><span class="line"><span class="comment">#将tensor传送到设备</span></span><br><span class="line">gpu_b = cpu_b.to(device)</span><br><span class="line">gpu_b.<span class="built_in">type</span>()		<span class="comment"># &#x27;torch.cuda.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="autograd自动求导">3 Autograd自动求导</h2>
<p>深度学习的算法本质上是通过反向传播求导数，而PyTorch的autograd模块则实现了此功能。在Tensor上的所有操作，autograd都能为它们自动提供微分，避免了手动计算导数的复杂过程。</p>
<h3 id="标量">3.1 标量</h3>
<p>我读了教程和官方文档，发现还是有个例子会更快理解。</p>
<blockquote>
<p><a
target="_blank" rel="noopener" href="https://www.jianshu.com/p/cbce2dd60120">超简单！pytorch入门教程（二）：Autograd</a></p>
</blockquote>
<p>这篇简书文章的例子有助于快速理解，值得一读。有趣的是，该文的例子中，函数值y和y对x在x1=1时的偏导均为4根号2，即5.6569。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = Variable(torch.ones(<span class="number">2</span>), requires_grad = <span class="literal">True</span>) <span class="comment"># Variable是tensor的一个外包装</span></span><br><span class="line">z = <span class="number">4</span>*x*x</span><br><span class="line">y = z.norm()	<span class="comment"># y最终为标量</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># Variable containing:</span></span><br><span class="line"><span class="comment"># 5.6569</span></span><br><span class="line"><span class="comment"># [torch.FloatTensor of size 1]</span></span><br><span class="line"></span><br><span class="line">y.backward()    <span class="comment"># backward()函数表示backprop</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)   <span class="comment"># 返回y关于x的梯度向量</span></span><br><span class="line"><span class="comment"># Variable containing:</span></span><br><span class="line"><span class="comment"># 5.6569</span></span><br><span class="line"><span class="comment"># 5.6569</span></span><br><span class="line"><span class="comment"># [torch.FloatTensor of size 2]</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，可以看到，当我们需要运行反向传播（Back
Propagation）算法时，直接调取<code>Variable.grad</code>即可得知其梯度值。这就是PyTorch的Autograd机制自动求导的结果。</p>
<p>注：从0.4起, Variable 正式合并入Tensor, Variable
本来实现的自动微分功能，Tensor就能支持。读者还是可以使用<code>Variable(tensor)</code>,
但是这个操作其实什么都没做。所以，以后的代码建议直接使用Tensor，因为官方文档中已经将Variable设置成过期模块要想使得Tensor使用autograd功能，只需要设置<code>tensor.requries_grad=True</code>。</p>
<h3 id="向量">3.2 向量</h3>
<p>上述的例子中，因为y是标量（scalar），所以<code>y.backward()</code>相当于<code>y.backward(torch.tensor(1))</code>。但如果<code>y</code>是向量时，<code>y.backward()</code>需要输入参数<code>grad_tensors</code>表示下降梯度向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y)		<span class="comment"># tensor([  491.3611,   545.1010, -1226.0724], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">gradients = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(gradients)	<span class="comment"># y是3维向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 另可：我们的返回值不是一个scalar，所以需要输入一个大小相同的张量作为参数，这里我们用ones_like函数根据x生成一个张量</span></span><br><span class="line"><span class="comment"># y.backward(torch.ones_like(x))</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)	<span class="comment"># tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span></span><br></pre></td></tr></table></figure>
<p>这篇文章做了详细的向量autograd的分析：</p>
<blockquote>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/witnessai1/article/details/79763596">Pytorch中的backward
- CSDN</a></p>
</blockquote>
<h3 id="禁用autograd">3.3 禁用autograd</h3>
<p>如果<code>.requires_grad=True</code>但是你又不希望进行autograd的计算，
那么可以将变量包裹在 <code>with torch.no_grad()</code>中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.requires_grad)			<span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)	<span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">	<span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)	<span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<p>这个方法在测试集测试准确率的时候回经常用到。</p>
<h2 id="神经网络">4 神经网络</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">torch.nn - PyTorch
master documentation</a></p>
</blockquote>
<p>详细文档参阅PyTorch官方文档关于<code>nn</code>包的信息。</p>
<h3 id="定义网络">4.1 定义网络</h3>
<p>使用<code>torch.nn</code>包来构建神经网络。</p>
<p><code>nn</code>包依赖<code>autograd</code>包来定义模型并求导。
一个<code>nn.Module</code>包含各个层和一个<code>forward(input)</code>方法，该方法返回<code>output</code>。</p>
<p>除了<code>nn</code>别名以外，我们还引用了<code>nn.functional</code>，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，<strong>不具有可学习的参数</strong>（如ReLU，pool，DropOut等），这些函数可以放在构造函数中，也可以不放，但是这里建议不放。</p>
<p>一般情况下我们会<strong>将nn.functional
设置为大写的F</strong>，这样缩写方便调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):		<span class="comment"># 必要：定义神经网络模型的结构</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)	<span class="comment"># 输入16个卷积核，各5×5像素</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):	<span class="comment"># 必要：定义神经网络模型的前向传播计算细节</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))		<span class="comment"># 卷积-&gt;激活-&gt;池化</span></span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line">    </span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Net(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=400, out_features=120, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=120, out_features=84, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc3): Linear(in_features=84, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<p><strong>在模型中必须要定义 <code>forward</code>
函数</strong>，<code>backward</code>
函数（用来计算梯度）会被<code>autograd</code>自动创建。 可以在
<code>forward</code> 函数中使用任何针对 Tensor 的操作。</p>
<p><code>net.parameters()</code>返回可被学习的参数（权重）列表和值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10</span></span><br><span class="line"><span class="comment"># torch.Size([6, 1, 5, 5])</span></span><br></pre></td></tr></table></figure>
<p>测试随机输入32×32。
注：这个网络（LeNet）期望的输入大小是32×32，如果使用MNIST数据集来训练这个网络，请把图片大小重新调整到32×32。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)	<span class="comment"># samples, channels, height, width</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[-0.0204, -0.0268, -0.0829,  0.1420, -0.0192,  0.1848,  0.0723, -0.0393,</span></span><br><span class="line"><span class="comment">#          -0.0275,  0.0867]], grad_fn=&lt;ThAddmmBackward&gt;)</span></span><br></pre></td></tr></table></figure>
<p>将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()		<span class="comment"># 否则梯度(.grad)会累加到已存在的梯度上</span></span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))	<span class="comment"># 此处还未定义损失函数，仅用out反向传播作为示例</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：<code>torch.nn</code> 只支持mini-batch输入</strong></p>
<p><strong><code>torch.nn</code> 只支持小批量输入。</strong>整个
<code>torch.nn</code> 包都只支持小批量样本，而不支持单个样本。
例如，<code>nn.Conv2d</code> 接受一个4维的张量，
<code>每一维分别是sSamples * nChannels * Height * Width（样本数*通道数*高*宽）</code>。
如果你有单个样本，只需使用 <code>input.unsqueeze(0)</code>
来添加其它的维数。</p>
<h3 id="损失函数">4.2 损失函数</h3>
<p>一个损失函数接受一对 (output, target)
作为输入，计算一个值来估计网络的输出和目标值相差多少。</p>
<p>output就是神经网络的输出结果，target就是数据的标记值。</p>
<p><code>nn</code>包中有很多不同的<a
target="_blank" rel="noopener" href="https://pytorch.org/docs/nn.html#loss-functions">损失函数</a>。
<code>nn.MSELoss</code>是一个比较简单的损失函数，它计算输出和目标间的<strong>均方误差</strong>，
例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)			<span class="comment"># 1×10 tensor</span></span><br><span class="line">target = torch.randn(<span class="number">10</span>)  	<span class="comment"># 随机值作为样例</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># 使target和output的shape相同，转换为1×10 tensor</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"><span class="comment"># tensor(1.3172, grad_fn=&lt;MseLossBackward&gt;)</span></span><br></pre></td></tr></table></figure>
<p>当反向传播计算<code>loss</code>时，读取<code>.grad_fn</code>属性，就可以看到一个图（graph）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>
<p>此时，如果调用<code>loss.backward()</code>函数，整个图都会去求<code>loss</code>的微分，图中属性<code>requires_grad=True</code>的张量的<code>.grad</code>属性会累计梯度。</p>
<p>反向几步查看这几步的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure>
<h3 id="反向传播">4.3 反向传播</h3>
<p>调用<code>loss.backward()</code>获得反向传播的误差。</p>
<p>但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。</p>
<p>现在，我们将调用<code>loss.backward()</code>，并查看<code>conv1</code>层的偏差（bias）项在反向传播前后的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># 清除梯度</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv1.bias.grad before backward</span></span><br><span class="line"><span class="comment"># tensor([0., 0., 0., 0., 0., 0.])        反向传播前梯度为0</span></span><br><span class="line"><span class="comment"># conv1.bias.grad after backward</span></span><br><span class="line"><span class="comment"># tensor([ 0.0074, -0.0249, -0.0107,  0.0326, -0.0017, -0.0059])	反向传播后各bias参数出现了梯度值</span></span><br></pre></td></tr></table></figure>
<h3 id="更新权重">4.4 更新权重</h3>
<p>在实践中最简单的权重更新规则是随机梯度下降（SGD）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>
<p>我们可以使用简单的Python代码实现这个规则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():      <span class="comment"># 遍历net对象中的所有参数进行更新</span></span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p>但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSProp等，PyTorch中构建了一个包<code>torch.optim</code>实现了所有的这些规则。
使用它们非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)	<span class="comment"># 以SGD为例</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> input_batches:</span><br><span class="line">    <span class="comment"># in your training loop:</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空累加的梯度值，和net.zero_grad()效果一致</span></span><br><span class="line">    output = net(<span class="built_in">input</span>)     <span class="comment"># 每次一个mini-batch的数据</span></span><br><span class="line">    loss = criterion(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>
<h2 id="数据加载与预处理">5 数据加载与预处理</h2>
<p>PyTorch通过<code>torch.utils.data</code>对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。
并且<code>torchvision</code>已经预先实现了常用图像数据集，包括CIFAR-10、ImageNet、COCO、MNIST、LSUN等数据集，可通过<code>torchvision.datasets</code>方便的调用。</p>
<h3 id="dataset">5.1 Dataset</h3>
<p><code>Dataset</code>是一个抽象类,
为了能够方便的读取，需要将要使用的数据包装为<code>Dataset</code>类。
自定义的<code>Dataset</code>需要继承它并且实现两个成员方法：</p>
<ol type="1">
<li><code>__getitem__()</code> 该方法定义每次怎么获取数据</li>
<li><code>__len__()</code> 该方法返回数据集的总长度</li>
</ol>
<p>下面我们使用kaggle上的一个竞赛<a
target="_blank" rel="noopener" href="https://www.kaggle.com/c/bluebook-for-bulldozers/data">bluebook
for
bulldozers</a>自定义一个数据集，为了方便介绍，我们使用里面的数据字典来做说明（因为条数少）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BulldozerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 数据集演示 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;实现初始化方法，在初始化的时候将数据读载入&quot;&quot;&quot;</span></span><br><span class="line">        self.df = pd.read_csv(csv_file)		<span class="comment"># 初始化DataFrame对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        返回df的长度</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        根据IDX返回一列数据</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.df.iloc[idx].SalePrice</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 实例化和使用数据集对象</span></span><br><span class="line">ds_demo = BulldozerDataset(<span class="string">&#x27;median_benchmark.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(ds_demo)	<span class="comment"># 实现了__len__方法所以可以直接使用len获取数据总数</span></span><br><span class="line">ds_demo[<span class="number">0</span>]		<span class="comment"># 用索引可以直接访问对应的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 11573</span></span><br><span class="line"><span class="comment"># 24000.0</span></span><br></pre></td></tr></table></figure>
<h3 id="dataloader">5.2 DataLoader</h3>
<blockquote>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a></p>
</blockquote>
<p><code>DataLoader</code>为我们提供了对<code>Dataset</code>的<strong>读取操作</strong>，常用参数有：</p>
<ul>
<li><code>batch_size</code> （每个batch的大小）</li>
<li><code>shuffle</code>
（是否进行shuffle操作，即在每轮epoch时是否对数据重新洗牌）</li>
<li><code>num_workers</code> （加载数据的时候使用几个子进程）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dl = torch.utils.data.DataLoader(ds_demo, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取单批数据示例</span></span><br><span class="line">idata = <span class="built_in">iter</span>(dl)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(idata))</span><br><span class="line"><span class="comment"># tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,</span></span><br><span class="line"><span class="comment">#         24000.], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有批数据示例</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dl):</span><br><span class="line">    <span class="built_in">print</span>(i,data)</span><br><span class="line">    <span class="keyword">break</span>	<span class="comment"># 这里只循环一遍</span></span><br><span class="line"><span class="comment"># 0 tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,</span></span><br><span class="line"><span class="comment">#         24000.], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>
<h3 id="torchvision">5.3 torchvision</h3>
<p><code>torchvision</code>是PyTorch中专门用来处理图像的库。</p>
<h4 id="torchvision.datasets">5.3.1 torchvision.datasets</h4>
<p><code>torchvision.datasets</code>可以理解为PyTorch团队自定义的<code>Dataset</code>，这些<code>Dataset</code>帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用：</p>
<ul>
<li>MNIST</li>
<li>COCO</li>
<li>Captions</li>
<li>Detection</li>
<li>LSUN</li>
<li>ImageFolder</li>
<li>Imagenet-12</li>
<li>CIFAR</li>
<li>STL10</li>
<li>SVHN</li>
<li>PhotoTour</li>
</ul>
<p>我们可以直接使用，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line">trainset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, <span class="comment"># 表示 MNIST 数据的加载的目录</span></span><br><span class="line">                        train=<span class="literal">True</span>,  <span class="comment"># 表示是否加载数据库的训练集，false的时候加载测试集</span></span><br><span class="line">                        download=<span class="literal">True</span>, <span class="comment"># 表示是否自动下载 MNIST 数据集</span></span><br><span class="line">                        transform=<span class="literal">None</span>) <span class="comment"># 表示是否需要对数据进行预处理，none为不进行预处理</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="torchvision.models">5.3.2 torchvision.models</h4>
<p><code>torchvision</code>不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习
<code>torchvision.models</code>模块的 子模块中包含以下模型结构。</p>
<ul>
<li>AlexNet</li>
<li>VGG</li>
<li>ResNet</li>
<li>SqueezeNet</li>
<li>DenseNet</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的</span></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="torchvision.transforms">5.3.3 torchvision.transforms</h4>
<p><code>transforms</code>模块提供了一般的图像转换操作类，用作数据的处理和增广。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> transforms</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),  <span class="comment">#先四周填充0，在吧图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.RandomRotation((-<span class="number">45</span>,<span class="number">45</span>)), <span class="comment">#随机旋转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>肯定有人会问：(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010)
这几个数字是什么意思？</p>
<blockquote>
<p>官方的这个帖子有详细的说明: <a
target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21"
class="uri">https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21</a>
这些都是根据ImageNet训练的归一化参数，可以直接使用，我们认为这个是固定值就可以</p>
</blockquote>
<h2 id="图像分类实例">6 图像分类实例</h2>
<p>基于CIFAR10数据集，实现一个图像分类器实例。</p>
<p>训练一个典型的图像分类分类器依次按照下列顺序进行：</p>
<ol type="1">
<li>使用<code>torchvision</code>加载和归一化CIFAR10训练集和测试集</li>
<li>定义一个卷积神经网络</li>
<li>定义损失函数</li>
<li>在训练集上训练网络</li>
<li>在测试集上测试网络</li>
</ol>
<h3 id="处理数据">6.1 处理数据</h3>
<p>一般情况下处理图像、文本、音频和视频数据时，可以使用标准的Python包来加载数据到一个numpy数组中。
然后把这个数组转换成 <code>torch.*Tensor</code>。</p>
<ul>
<li>图像可以使用 Pillow, OpenCV</li>
<li>音频可以使用 scipy, librosa</li>
<li>文本可以使用原始Python和Cython来加载，或者使用 NLTK或 SpaCy
处理</li>
</ul>
<p>特别的，对于图像任务，我们创建了一个包
<code>torchvision</code>，它包含了处理一些基本图像数据集的方法。这些数据集包括
Imagenet, CIFAR10, MNIST 等。除了数据加载以外，<code>torchvision</code>
还包含了图像转换器， <code>torchvision.datasets</code> 和
<code>torch.utils.data.DataLoader</code>。</p>
<p><code>torchvision</code>包不仅提供了巨大的便利，也避免了代码的重复。</p>
<p>在这个教程中，我们使用CIFAR10数据集，它有如下10个类别 ：‘airplane’,
‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’,
‘truck’。CIFAR-10的图像都是
3x32x32大小的，即，3颜色通道，32x32像素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)		<span class="comment"># 配置CIFAR10训练集</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)			<span class="comment"># 配置数据加载器，每个mini-batch含4张图</span></span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)		<span class="comment"># 配置CIFAR10测试集</span></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)			<span class="comment"># 配置数据加载器，测试时也是4个图一个mini-batch</span></span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)	<span class="comment"># 共10分类</span></span><br></pre></td></tr></table></figure>
<p>可以通过<code>matplotlib</code>库来绘图查看数据集中的图片</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图像的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取随机数据</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)	<span class="comment"># 配置迭代器从数据加载器中读取数据</span></span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()	<span class="comment"># 读取下一项数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图像</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))		<span class="comment"># make_grid把mini-batch的图片张量变成图片网格</span></span><br><span class="line"><span class="comment"># 显示图像标签</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="定义卷积神经网络模型">6.2 定义卷积神经网络模型</h3>
<p>从之前的神经网络一节复制神经网络代码，并修改为输入3通道图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)		<span class="comment"># 3通道（彩色图片），6个5×5的卷积核</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数与优化器">6.3 定义损失函数与优化器</h3>
<p>我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()		<span class="comment"># 交叉熵损失函数</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练神经网络模型">6.4 训练神经网络模型</h3>
<p>有趣的时刻开始了。
我们只需在数据迭代器上循环，将数据输入给网络，并优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># 多轮（epoch）训练</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):	<span class="comment"># 多mini-batch循环</span></span><br><span class="line">        <span class="comment"># 获取输入</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度置0（清空累加grad）</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正向传播，反向传播，优化</span></span><br><span class="line">        outputs = net(inputs)				<span class="comment"># 模型正向传播，inputs=&gt;outputs</span></span><br><span class="line">        loss = criterion(outputs, labels)	<span class="comment"># 调用交叉熵损失</span></span><br><span class="line">        loss.backward()						<span class="comment"># 对损失进行反向传播</span></span><br><span class="line">        optimizer.step()					<span class="comment"># 对参数梯度下降一步</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印状态信息</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># 每2000批次打印一次</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span>	<span class="comment"># running_loss是2000个batch的loss和</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试神经网络模型">6.5 测试神经网络模型</h3>
<p>我们在整个训练集上进行了2轮（epoch）训练，但是我们需要检查网络是否从数据集中学习到有用的东西。
通过预测神经网络输出的类别标签与实际情况标签进行对比来进行检测。
如果预测正确，我们把该样本添加到正确预测列表。
第一步，显示测试集中的图片并熟悉图片内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(testloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对模型输入图片</span></span><br><span class="line">outputs = net(images)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出是10个标签的分值。</span></span><br><span class="line"><span class="comment"># 一个类别的分值越大，神经网络越认为它是这个类别。所以让我们得到最高能量的标签。</span></span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))	<span class="comment"># 列表推导式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计模型在整个测试集上的测试结果</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():	<span class="comment"># 禁用梯度计算</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:		<span class="comment"># 加载测试数据</span></span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)	<span class="comment"># 正向传播</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)	<span class="comment"># 计算mini-batch的预测结果</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)	<span class="comment"># 统计已测试图片个数</span></span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()	<span class="comment"># 统计预测正确的个数</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy of the network on the 10000 test images: 9 %</span></span><br></pre></td></tr></table></figure>
<p>结果看起来不错，至少比随机选择要好，随机选择的正确率为10%。
似乎网络学习到了一些东西。</p>
<p>在识别哪一个类的时候好，哪一个不好呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class_correct = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">class_total = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()		<span class="comment"># 统计每类的正确数</span></span><br><span class="line">            class_total[label] += <span class="number">1</span>					<span class="comment"># 统计每类的预测总数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy of %5s : %2d %%&#x27;</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy of plane : 99 %</span></span><br><span class="line"><span class="comment"># Accuracy of   car :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of  bird :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of   cat :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of  deer :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of   dog :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of  frog :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of horse :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of  ship :  0 %</span></span><br><span class="line"><span class="comment"># Accuracy of truck :  0 %</span></span><br></pre></td></tr></table></figure>
<h3 id="在gpu上训练">6.6 在GPU上训练</h3>
<h4 id="检查gpu支持">6.6.1 检查GPU支持</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># 确认我们的电脑支持CUDA，然后显示CUDA信息：</span></span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># device(type=&#x27;cuda&#x27;, index=0)</span></span><br></pre></td></tr></table></figure>
<p>本节的其余部分假定<code>device</code>是CUDA设备。</p>
<h4 id="神经网络模型载入cuda">6.6.2 神经网络模型载入CUDA</h4>
<p><code>torch.nn.Module.to</code>将递归遍历所有模块并将模块的参数和缓冲区转换成CUDA张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)		<span class="comment"># 把神经网络模型载入CUDA</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html?highlight=#torch.nn.Module.to">torch.nn.Module.to</a></p>
</blockquote>
<h4 id="输入数据载入cuda">6.6.3 输入数据载入CUDA</h4>
<p>记住：inputs 和 targets 也要转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html?highlight=#torch.Tensor.to">torch.Tensor.to</a></p>
</blockquote>
<p>为什么我们没注意到GPU的速度提升很多？那是因为网络非常的小。</p>
<p><strong>实践:</strong>
尝试增加你的网络的宽度（第一个<code>nn.Conv2d</code>的第2个参数，第二个<code>nn.Conv2d</code>的第一个参数，它们需要是相同的数字），看看你得到了什么样的加速。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>支持我的写作！(Support my writing!)</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="Heary 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Heary 支付宝">
        <span>支付宝</span>
      </div>
      <div>
        <img src="/images/paypal.png" alt="Heary PayPal">
        <span>PayPal</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Heary
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://heary.cn/posts/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="PyTorch学习笔记">https://heary.cn/posts/PyTorch学习笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a>
              <a href="/tags/PyTorch/" rel="tag"><i class="fa fa-tag"></i> PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/Cascade-R-CNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" rel="prev" title="Cascade R-CNN论文阅读笔记">
                  <i class="fa fa-chevron-left"></i> Cascade R-CNN论文阅读笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/mmdetection-%E5%9F%BA%E4%BA%8EPyTorch%E7%9A%84%E5%BC%80%E6%BA%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F/" rel="next" title="mmdetection - 基于PyTorch的开源目标检测系统">
                  mmdetection - 基于PyTorch的开源目标检测系统 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Heary</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">693k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">10:30</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.2/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"HearyShen","repo":"gitalk-comments","client_id":"77d476596d65dea261b8","client_secret":"225837e23b2eb099dc77a3aa38dbeefffef4599b","admin_user":"HearyShen","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.2/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"001317edb5830eb00768a9e1bb684cf2"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
